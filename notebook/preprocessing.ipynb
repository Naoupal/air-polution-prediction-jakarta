{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02dbbf88",
   "metadata": {},
   "source": [
    "# Air Quality Prediction - Preprocessing Pipeline\n",
    "## Jakarta ISPU Data (2022-2025)\n",
    "\n",
    "This notebook creates a comprehensive preprocessing pipeline for air quality prediction, including:\n",
    "1. **Temporal Filtering & Population Extrapolation** - Filter data to 2022-2025 and extrapolate population\n",
    "2. **Spatial Mapping** - Join river quality data using Haversine distance\n",
    "3. **Advanced Feature Engineering** - Wind direction encoding, lag features, rolling windows\n",
    "4. **Data Cleaning & Encoding** - Handle missing values, outliers, and encode target variable\n",
    "5. **Final Output** - Merge all features and apply StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62cf2589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully\n",
      "✓ Data path: ../penyisihan-datavidia-10/\n",
      "✓ Target years: 2022-2025\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 1: Import Libraries and Configuration\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import warnings\n",
    "import os\n",
    "import glob\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "DATA_PATH = \"../penyisihan-datavidia-10/\"\n",
    "START_YEAR = 2022\n",
    "END_YEAR = 2025\n",
    "\n",
    "# Station coordinates (latitude, longitude) for Jakarta ISPU stations\n",
    "# These are approximate coordinates for each monitoring station\n",
    "STATION_COORDS = {\n",
    "    'DKI1': {'name': 'Bundaran HI', 'lat': -6.1944, 'lon': 106.8229, 'area': 'Jakarta Pusat'},\n",
    "    'DKI2': {'name': 'Kelapa Gading', 'lat': -6.1616, 'lon': 106.9070, 'area': 'Jakarta Utara'},\n",
    "    'DKI3': {'name': 'Jagakarsa', 'lat': -6.3339, 'lon': 106.8235, 'area': 'Jakarta Selatan'},\n",
    "    'DKI4': {'name': 'Lubang Buaya', 'lat': -6.2914, 'lon': 106.9017, 'area': 'Jakarta Timur'},\n",
    "    'DKI5': {'name': 'Kebon Jeruk', 'lat': -6.1886, 'lon': 106.7633, 'area': 'Jakarta Barat'}\n",
    "}\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")\n",
    "print(f\"✓ Data path: {DATA_PATH}\")\n",
    "print(f\"✓ Target years: {START_YEAR}-{END_YEAR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c207497c",
   "metadata": {},
   "source": [
    "## Section 1: Load All Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "101fe6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ISPU data loaded: 16,686 records\n",
      "  Date range: 2010-01-01 to 2025-08-31\n",
      "  Stations: ['DKI1', 'DKI2', 'DKI3', 'DKI4', 'DKI5']\n",
      "  Years available: [np.int32(2010), np.int32(2011), np.int32(2012), np.int32(2013), np.int32(2014), np.int32(2015), np.int32(2016), np.int32(2017), np.int32(2018), np.int32(2019), np.int32(2020), np.int32(2021), np.int32(2022), np.int32(2023), np.int32(2024), np.int32(2025)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tanggal</th>\n",
       "      <th>stasiun_id</th>\n",
       "      <th>stasiun</th>\n",
       "      <th>pm_sepuluh</th>\n",
       "      <th>pm_duakomalima</th>\n",
       "      <th>sulfur_dioksida</th>\n",
       "      <th>karbon_monoksida</th>\n",
       "      <th>ozon</th>\n",
       "      <th>nitrogen_dioksida</th>\n",
       "      <th>max</th>\n",
       "      <th>parameter_pencemar_kritis</th>\n",
       "      <th>kategori</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>DKI1</td>\n",
       "      <td>DKI1 (Bunderan HI)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>73</td>\n",
       "      <td>27</td>\n",
       "      <td>14</td>\n",
       "      <td>73</td>\n",
       "      <td>CO</td>\n",
       "      <td>SEDANG</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-02</td>\n",
       "      <td>DKI1</td>\n",
       "      <td>DKI1 (Bunderan HI)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>33</td>\n",
       "      <td>9</td>\n",
       "      <td>33</td>\n",
       "      <td>O3</td>\n",
       "      <td>BAIK</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-03</td>\n",
       "      <td>DKI1</td>\n",
       "      <td>DKI1 (Bunderan HI)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>27</td>\n",
       "      <td>PM10</td>\n",
       "      <td>BAIK</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>DKI1</td>\n",
       "      <td>DKI1 (Bunderan HI)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>PM10</td>\n",
       "      <td>BAIK</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>DKI1</td>\n",
       "      <td>DKI1 (Bunderan HI)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>25</td>\n",
       "      <td>PM10</td>\n",
       "      <td>BAIK</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tanggal stasiun_id             stasiun pm_sepuluh pm_duakomalima  \\\n",
       "0 2010-01-01       DKI1  DKI1 (Bunderan HI)        NaN            NaN   \n",
       "1 2010-01-02       DKI1  DKI1 (Bunderan HI)        NaN            NaN   \n",
       "2 2010-01-03       DKI1  DKI1 (Bunderan HI)        NaN            NaN   \n",
       "3 2010-01-04       DKI1  DKI1 (Bunderan HI)        NaN            NaN   \n",
       "4 2010-01-05       DKI1  DKI1 (Bunderan HI)        NaN            NaN   \n",
       "\n",
       "  sulfur_dioksida karbon_monoksida ozon nitrogen_dioksida max  \\\n",
       "0               4               73   27                14  73   \n",
       "1               2               16   33                 9  33   \n",
       "2               2               19   20                 9  27   \n",
       "3               2               16   15                 6  22   \n",
       "4               2               17   15                 8  25   \n",
       "\n",
       "  parameter_pencemar_kritis kategori  year  \n",
       "0                        CO   SEDANG  2010  \n",
       "1                        O3     BAIK  2010  \n",
       "2                      PM10     BAIK  2010  \n",
       "3                      PM10     BAIK  2010  \n",
       "4                      PM10     BAIK  2010  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 2: Load and Combine ISPU Data (2022-2025)\n",
    "# =============================================================================\n",
    "\n",
    "def load_ispu_data(data_path):\n",
    "    \"\"\"\n",
    "    Load and combine all ISPU (Air Quality Index) data files from 2022-2025.\n",
    "    Standardizes column names across different file formats.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Combined ISPU data with standardized columns\n",
    "    \"\"\"\n",
    "    ispu_files = glob.glob(os.path.join(data_path, \"ISPU\", \"*.csv\"))\n",
    "    \n",
    "    dfs = []\n",
    "    for file in ispu_files:\n",
    "        df = pd.read_csv(file)\n",
    "        \n",
    "        # Standardize column names across different file formats\n",
    "        col_mapping = {\n",
    "            'pm_10': 'pm_sepuluh',\n",
    "            'so2': 'sulfur_dioksida',\n",
    "            'co': 'karbon_monoksida',\n",
    "            'o3': 'ozon',\n",
    "            'no2': 'nitrogen_dioksida',\n",
    "            'critical': 'parameter_pencemar_kritis',\n",
    "            'categori': 'kategori',\n",
    "            'lokasi_spku': 'stasiun'\n",
    "        }\n",
    "        df.rename(columns=col_mapping, inplace=True)\n",
    "        \n",
    "        # Handle the 'tanggal' column which may be in different formats\n",
    "        # For 2024 and 2025 files, 'tanggal' is just the day number and 'bulan' is the month\n",
    "        if 'bulan' in df.columns and 'tanggal' in df.columns:\n",
    "            # Construct date from periode_data (YYYYMM), bulan, and tanggal (day)\n",
    "            df['year'] = df['periode_data'].astype(str).str[:4].astype(int)\n",
    "            df['month'] = df['bulan'].astype(int)\n",
    "            df['day'] = df['tanggal'].astype(int)\n",
    "            df['tanggal'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "        elif 'tanggal' in df.columns:\n",
    "            # Handle Excel serial date format (numeric) and string dates\n",
    "            # Try to convert - if it's a number > 40000, it's likely an Excel serial date\n",
    "            def parse_date(val):\n",
    "                if pd.isna(val):\n",
    "                    return pd.NaT\n",
    "                if isinstance(val, (int, float)) and val > 40000:\n",
    "                    # Excel serial date - days since 1899-12-30\n",
    "                    try:\n",
    "                        return pd.Timestamp('1899-12-30') + pd.Timedelta(days=int(val))\n",
    "                    except:\n",
    "                        return pd.NaT\n",
    "                try:\n",
    "                    return pd.to_datetime(val)\n",
    "                except:\n",
    "                    return pd.NaT\n",
    "            \n",
    "            df['tanggal'] = df['tanggal'].apply(parse_date)\n",
    "        \n",
    "        # Extract station ID from station name if needed\n",
    "        if 'stasiun' in df.columns:\n",
    "            df['stasiun_id'] = df['stasiun'].astype(str).str.extract(r'(DKI\\d)')[0]\n",
    "        \n",
    "        dfs.append(df)\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    df_ispu = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Extract station_id for consistent naming\n",
    "    if 'stasiun_id' not in df_ispu.columns:\n",
    "        df_ispu['stasiun_id'] = df_ispu['stasiun'].astype(str).str.extract(r'(DKI\\d)')[0]\n",
    "    \n",
    "    # Ensure tanggal is datetime\n",
    "    df_ispu['tanggal'] = pd.to_datetime(df_ispu['tanggal'], errors='coerce')\n",
    "    \n",
    "    # Drop rows with invalid dates\n",
    "    df_ispu = df_ispu.dropna(subset=['tanggal'])\n",
    "    \n",
    "    # Extract year for filtering\n",
    "    df_ispu['year'] = df_ispu['tanggal'].dt.year\n",
    "    \n",
    "    # Select and order relevant columns\n",
    "    cols_to_keep = ['tanggal', 'stasiun_id', 'stasiun', 'pm_sepuluh', 'pm_duakomalima',\n",
    "                    'sulfur_dioksida', 'karbon_monoksida', 'ozon', 'nitrogen_dioksida',\n",
    "                    'max', 'parameter_pencemar_kritis', 'kategori', 'year']\n",
    "    \n",
    "    existing_cols = [col for col in cols_to_keep if col in df_ispu.columns]\n",
    "    df_ispu = df_ispu[existing_cols]\n",
    "    \n",
    "    # Drop duplicates based on date and station\n",
    "    df_ispu = df_ispu.drop_duplicates(subset=['tanggal', 'stasiun_id'])\n",
    "    \n",
    "    return df_ispu.sort_values(['stasiun_id', 'tanggal']).reset_index(drop=True)\n",
    "\n",
    "# Load ISPU data\n",
    "df_ispu = load_ispu_data(DATA_PATH)\n",
    "print(f\"✓ ISPU data loaded: {df_ispu.shape[0]:,} records\")\n",
    "print(f\"  Date range: {df_ispu['tanggal'].min().date()} to {df_ispu['tanggal'].max().date()}\")\n",
    "print(f\"  Stations: {sorted([s for s in df_ispu['stasiun_id'].unique() if pd.notna(s)])}\")\n",
    "print(f\"  Years available: {sorted(df_ispu['year'].unique())}\")\n",
    "df_ispu.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3285a8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Weather data loaded: 28,610 records\n",
      "  Date range: 2010-01-01 to 2025-08-31\n",
      "  Stations: ['DKI1', 'DKI2', 'DKI3', 'DKI4', 'DKI5']\n",
      "  Columns: ['tanggal', 'temp_max', 'temp_min', 'precipitation_sum', 'precipitation_hours', 'wind_speed_max', 'wind_direction_10m_dominant', 'radiation_sum', 'temp_mean', 'humidity_mean', 'cloud_cover_mean', 'pressure_mean', 'wind_gusts_max', 'wind_direction_alt', 'humidity_max', 'humidity_min', 'cloud_cover_max', 'cloud_cover_min', 'wind_gusts_mean', 'wind_speed_mean', 'wind_gusts_min', 'wind_speed_min', 'pressure_max', 'pressure_min', 'stasiun_id', 'year']\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 3: Load Weather Data (All Stations)\n",
    "# =============================================================================\n",
    "\n",
    "def load_weather_data(data_path):\n",
    "    \"\"\"\n",
    "    Load and combine weather data from all 5 Jakarta monitoring stations.\n",
    "    Maps file names to station IDs for joining with ISPU data.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Combined weather data with station IDs\n",
    "    \"\"\"\n",
    "    weather_files = glob.glob(os.path.join(data_path, \"cuaca-harian\", \"*.csv\"))\n",
    "    \n",
    "    # Mapping of file patterns to station IDs\n",
    "    station_file_mapping = {\n",
    "        'dki1': 'DKI1',\n",
    "        'dki2': 'DKI2', \n",
    "        'dki3': 'DKI3',\n",
    "        'dki4': 'DKI4',\n",
    "        'dki5': 'DKI5'\n",
    "    }\n",
    "    \n",
    "    dfs = []\n",
    "    for file in weather_files:\n",
    "        df = pd.read_csv(file)\n",
    "        \n",
    "        # Determine station ID from filename\n",
    "        filename_lower = os.path.basename(file).lower()\n",
    "        station_id = None\n",
    "        for pattern, sid in station_file_mapping.items():\n",
    "            if pattern in filename_lower:\n",
    "                station_id = sid\n",
    "                break\n",
    "        \n",
    "        if station_id:\n",
    "            df['stasiun_id'] = station_id\n",
    "            \n",
    "            # Rename time column to tanggal for consistency\n",
    "            if 'time' in df.columns:\n",
    "                df.rename(columns={'time': 'tanggal'}, inplace=True)\n",
    "            \n",
    "            df['tanggal'] = pd.to_datetime(df['tanggal'], errors='coerce')\n",
    "            dfs.append(df)\n",
    "    \n",
    "    df_weather = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Simplify column names (remove units)\n",
    "    col_rename = {\n",
    "        'temperature_2m_max (°C)': 'temp_max',\n",
    "        'temperature_2m_min (°C)': 'temp_min',\n",
    "        'temperature_2m_mean (°C)': 'temp_mean',\n",
    "        'precipitation_sum (mm)': 'precipitation_sum',\n",
    "        'precipitation_hours (h)': 'precipitation_hours',\n",
    "        'wind_speed_10m_max (km/h)': 'wind_speed_max',\n",
    "        'wind_speed_10m_mean (km/h)': 'wind_speed_mean',\n",
    "        'wind_speed_10m_min (km/h)': 'wind_speed_min',\n",
    "        'wind_direction_10m_dominant (°)': 'wind_direction_10m_dominant',\n",
    "        'winddirection_10m_dominant (°)': 'wind_direction_alt',\n",
    "        'shortwave_radiation_sum (MJ/m²)': 'radiation_sum',\n",
    "        'relative_humidity_2m_mean (%)': 'humidity_mean',\n",
    "        'relative_humidity_2m_max (%)': 'humidity_max',\n",
    "        'relative_humidity_2m_min (%)': 'humidity_min',\n",
    "        'cloud_cover_mean (%)': 'cloud_cover_mean',\n",
    "        'cloud_cover_max (%)': 'cloud_cover_max',\n",
    "        'cloud_cover_min (%)': 'cloud_cover_min',\n",
    "        'surface_pressure_mean (hPa)': 'pressure_mean',\n",
    "        'surface_pressure_max (hPa)': 'pressure_max',\n",
    "        'surface_pressure_min (hPa)': 'pressure_min',\n",
    "        'wind_gusts_10m_max (km/h)': 'wind_gusts_max',\n",
    "        'wind_gusts_10m_mean (km/h)': 'wind_gusts_mean',\n",
    "        'wind_gusts_10m_min (km/h)': 'wind_gusts_min'\n",
    "    }\n",
    "    df_weather.rename(columns=col_rename, inplace=True)\n",
    "    \n",
    "    # Use primary wind direction column or fallback\n",
    "    if 'wind_direction_10m_dominant' not in df_weather.columns and 'wind_direction_alt' in df_weather.columns:\n",
    "        df_weather['wind_direction_10m_dominant'] = df_weather['wind_direction_alt']\n",
    "    \n",
    "    df_weather['year'] = df_weather['tanggal'].dt.year\n",
    "    \n",
    "    return df_weather.sort_values(['stasiun_id', 'tanggal']).reset_index(drop=True)\n",
    "\n",
    "# Load weather data\n",
    "df_weather = load_weather_data(DATA_PATH)\n",
    "print(f\"✓ Weather data loaded: {df_weather.shape[0]:,} records\")\n",
    "print(f\"  Date range: {df_weather['tanggal'].min().date()} to {df_weather['tanggal'].max().date()}\")\n",
    "print(f\"  Stations: {sorted(df_weather['stasiun_id'].unique())}\")\n",
    "print(f\"  Columns: {list(df_weather.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "561ab077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Population data loaded: 176 records\n",
      "  Years: [np.int64(2013), np.int64(2014), np.int64(2015), np.int64(2016)]\n",
      "  Kecamatan count: 44\n",
      "\n",
      "✓ NDVI data loaded: 1,810 records\n",
      "  Date range: 2009-12-19 to 2025-08-29\n",
      "\n",
      "✓ River quality data loaded: 480 records\n",
      "  Unique sampling points: 120\n",
      "  Parameters: ['Amoniak', 'BOD', 'COD', 'Cd', 'Cr6', 'Cu', 'DO', 'F', 'Fecal Coliform', 'Fenol', 'H2S', 'Hg', 'Klorida', 'Klorin Bebas', 'Klorin Bebas n In Situ', 'MBAS', 'Minyak dan Lemak', 'Ni', 'Nitrat', 'Nitrit', 'Pb', 'Sianida', 'Sulfat', 'TDS', 'TSS', 'Total Coliform', 'Total N', 'Total P', 'Warna', 'Zn', 'pH']\n",
      "\n",
      "✓ Holiday data loaded: 5,844 records\n",
      "  Date range: 2010-01-01 to 2025-12-31\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 4: Load Population, NDVI, River Quality, and Holiday Data\n",
    "# =============================================================================\n",
    "\n",
    "def load_population_data(data_path):\n",
    "    \"\"\"\n",
    "    Load population data by kelurahan/kecamatan for 2013-2016.\n",
    "    Aggregates by kecamatan and year for later extrapolation.\n",
    "    \"\"\"\n",
    "    pop_file = os.path.join(data_path, \"jumlah-penduduk\", \n",
    "                            \"data-jumlah-penduduk-provinsi-dki-jakarta-berdasarkan-kelompok-usia-dan-jenis-kelamin-tahun-2013-2021-komponen-data.csv\")\n",
    "    df_pop = pd.read_csv(pop_file)\n",
    "    \n",
    "    # Aggregate total population by kecamatan and year\n",
    "    df_pop_agg = df_pop.groupby(['tahun', 'nama_kabupaten_kota', 'nama_kecamatan']).agg({\n",
    "        'jumlah_penduduk': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    return df_pop_agg\n",
    "\n",
    "def load_ndvi_data(data_path):\n",
    "    \"\"\"\n",
    "    Load NDVI (Normalized Difference Vegetation Index) data.\n",
    "    Higher NDVI indicates more vegetation, which can improve air quality.\n",
    "    \"\"\"\n",
    "    ndvi_file = os.path.join(data_path, \"NDVI (vegetation index)\", \"indeks-ndvi-jakarta.csv\")\n",
    "    df_ndvi = pd.read_csv(ndvi_file)\n",
    "    df_ndvi['tanggal'] = pd.to_datetime(df_ndvi['tanggal'], errors='coerce')\n",
    "    df_ndvi['year'] = df_ndvi['tanggal'].dt.year\n",
    "    \n",
    "    return df_ndvi\n",
    "\n",
    "def load_river_data(data_path):\n",
    "    \"\"\"\n",
    "    Load river water quality data with coordinates.\n",
    "    Contains chemical parameters like pH, BOD, COD, DO, etc.\n",
    "    \"\"\"\n",
    "    river_file = os.path.join(data_path, \"kualitas-air-sungai\", \"data-kualitas-air-sungai-komponen-data.csv\")\n",
    "    df_river = pd.read_csv(river_file)\n",
    "    \n",
    "    # Pivot to get one row per sampling point with all parameters as columns\n",
    "    # First, get unique sampling points\n",
    "    df_river_pivot = df_river.pivot_table(\n",
    "        index=['periode_data', 'periode_pemantauan', 'bulan_sampling', 'titik_sampel', \n",
    "               'nama_sungai', 'alamat', 'latitude', 'longitude'],\n",
    "        columns='parameter',\n",
    "        values='hasil_pengukuran',\n",
    "        aggfunc='first'\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Flatten column names\n",
    "    df_river_pivot.columns = [str(col) if isinstance(col, str) else col for col in df_river_pivot.columns]\n",
    "    \n",
    "    return df_river_pivot\n",
    "\n",
    "def load_holiday_data(data_path):\n",
    "    \"\"\"\n",
    "    Load national holiday and weekend data.\n",
    "    Contains flags for weekends and national holidays.\n",
    "    \"\"\"\n",
    "    holiday_file = os.path.join(data_path, \"libur-nasional\", \"dataset-libur-nasional-dan-weekend.csv\")\n",
    "    df_holidays = pd.read_csv(holiday_file)\n",
    "    df_holidays['tanggal'] = pd.to_datetime(df_holidays['tanggal'], errors='coerce')\n",
    "    df_holidays['year'] = df_holidays['tanggal'].dt.year\n",
    "    \n",
    "    return df_holidays\n",
    "\n",
    "# Load all supporting datasets\n",
    "df_population = load_population_data(DATA_PATH)\n",
    "df_ndvi = load_ndvi_data(DATA_PATH)\n",
    "df_river = load_river_data(DATA_PATH)\n",
    "df_holidays = load_holiday_data(DATA_PATH)\n",
    "\n",
    "print(f\"✓ Population data loaded: {df_population.shape[0]:,} records\")\n",
    "print(f\"  Years: {sorted(df_population['tahun'].unique())}\")\n",
    "print(f\"  Kecamatan count: {df_population['nama_kecamatan'].nunique()}\")\n",
    "\n",
    "print(f\"\\n✓ NDVI data loaded: {df_ndvi.shape[0]:,} records\")\n",
    "print(f\"  Date range: {df_ndvi['tanggal'].min().date()} to {df_ndvi['tanggal'].max().date()}\")\n",
    "\n",
    "print(f\"\\n✓ River quality data loaded: {df_river.shape[0]:,} records\")\n",
    "print(f\"  Unique sampling points: {df_river['titik_sampel'].nunique()}\")\n",
    "print(f\"  Parameters: {df_river.columns.tolist()[8:]}\")\n",
    "\n",
    "print(f\"\\n✓ Holiday data loaded: {df_holidays.shape[0]:,} records\")\n",
    "print(f\"  Date range: {df_holidays['tanggal'].min().date()} to {df_holidays['tanggal'].max().date()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d527b42b",
   "metadata": {},
   "source": [
    "## Section 2: Temporal Filtering & Population Extrapolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0577549f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ISPU data filtered: 5,176 records (2022-2025)\n",
      "  Date range: 2022-01-01 to 2025-08-31\n",
      "\n",
      "✓ Weather data filtered: 6,695 records (2022-2025)\n",
      "  Date range: 2022-01-01 to 2025-08-31\n",
      "\n",
      "✓ Holiday data filtered: 1,461 records (2022-2025)\n",
      "  Date range: 2022-01-01 to 2025-12-31\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 5: Temporal Filtering Functions\n",
    "# =============================================================================\n",
    "\n",
    "def filter_by_year_range(df, start_year, end_year, date_col='tanggal', year_col='year'):\n",
    "    \"\"\"\n",
    "    Filter dataframe to only include data within specified year range.\n",
    "    \n",
    "    Parameters:\n",
    "        df: Input DataFrame\n",
    "        start_year: Start year (inclusive)\n",
    "        end_year: End year (inclusive)\n",
    "        date_col: Name of date column\n",
    "        year_col: Name of year column (will be created if not exists)\n",
    "    \n",
    "    Returns:\n",
    "        Filtered DataFrame\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Ensure year column exists\n",
    "    if year_col not in df.columns and date_col in df.columns:\n",
    "        df[year_col] = pd.to_datetime(df[date_col]).dt.year\n",
    "    \n",
    "    # Filter by year range\n",
    "    mask = (df[year_col] >= start_year) & (df[year_col] <= end_year)\n",
    "    filtered_df = df[mask].copy()\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "# Apply temporal filtering to datasets\n",
    "df_ispu_filtered = filter_by_year_range(df_ispu, START_YEAR, END_YEAR)\n",
    "df_weather_filtered = filter_by_year_range(df_weather, START_YEAR, END_YEAR)\n",
    "df_holidays_filtered = filter_by_year_range(df_holidays, START_YEAR, END_YEAR, year_col='year')\n",
    "\n",
    "print(f\"✓ ISPU data filtered: {df_ispu_filtered.shape[0]:,} records ({START_YEAR}-{END_YEAR})\")\n",
    "print(f\"  Date range: {df_ispu_filtered['tanggal'].min().date()} to {df_ispu_filtered['tanggal'].max().date()}\")\n",
    "\n",
    "print(f\"\\n✓ Weather data filtered: {df_weather_filtered.shape[0]:,} records ({START_YEAR}-{END_YEAR})\")\n",
    "print(f\"  Date range: {df_weather_filtered['tanggal'].min().date()} to {df_weather_filtered['tanggal'].max().date()}\")\n",
    "\n",
    "print(f\"\\n✓ Holiday data filtered: {df_holidays_filtered.shape[0]:,} records ({START_YEAR}-{END_YEAR})\")\n",
    "print(f\"  Date range: {df_holidays_filtered['tanggal'].min().date()} to {df_holidays_filtered['tanggal'].max().date()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9d34afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Population extrapolated for years: [2022, 2023, 2024, 2025]\n",
      "  Total records: 24\n",
      "  Cities: ['JAKARTA BARAT', 'JAKARTA PUSAT', 'JAKARTA SELATAN', 'JAKARTA TIMUR', 'JAKARTA UTARA', 'KAB.ADM.KEP.SERIBU']\n",
      "\n",
      "  Extrapolated population by city:\n",
      "tahun                   2022     2023     2024     2025\n",
      "nama_kabupaten_kota                                    \n",
      "JAKARTA BARAT        2332364  2332364  2332364  2332364\n",
      "JAKARTA PUSAT        1311376  1328565  1345755  1362945\n",
      "JAKARTA SELATAN      2699172  2767771  2836369  2904968\n",
      "JAKARTA TIMUR        2442570  2382158  2321747  2261335\n",
      "JAKARTA UTARA        1688957  1688957  1688957  1688957\n",
      "KAB.ADM.KEP.SERIBU     35876    36909    37941    38974\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 6: Population Extrapolation using Linear Regression\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "EXTRAPOLATION LOGIC:\n",
    "--------------------\n",
    "Population data is only available for 2013-2016. To estimate population for 2022-2025,\n",
    "we use LINEAR EXTRAPOLATION at the CITY (kabupaten/kota) level:\n",
    "\n",
    "1. Aggregate population by city and year first\n",
    "2. For each city, fit a linear regression model: Population = a * Year + b\n",
    "3. Use this model to predict population for target years (2022-2025)\n",
    "4. This approach is more stable than kecamatan-level extrapolation\n",
    "\n",
    "Note: Linear extrapolation over a large time gap (6+ years) may introduce errors.\n",
    "For production models, consider using official population projections from BPS.\n",
    "The erratic historical data (non-monotonic) suggests data quality issues which\n",
    "we handle by using city-level aggregation for more stable trends.\n",
    "\"\"\"\n",
    "\n",
    "def extrapolate_population_by_city(df_pop, target_years):\n",
    "    \"\"\"\n",
    "    Extrapolate population for target years using linear regression at city level.\n",
    "    \n",
    "    Parameters:\n",
    "        df_pop: Population DataFrame with columns [tahun, nama_kabupaten_kota, jumlah_penduduk]\n",
    "        target_years: List of years to predict population for\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with extrapolated population for each city\n",
    "    \"\"\"\n",
    "    # First aggregate to city level\n",
    "    df_city = df_pop.groupby(['tahun', 'nama_kabupaten_kota']).agg({\n",
    "        'jumlah_penduduk': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    extrapolated_data = []\n",
    "    \n",
    "    for kota in df_city['nama_kabupaten_kota'].unique():\n",
    "        # Get historical data for this city\n",
    "        city_data = df_city[df_city['nama_kabupaten_kota'] == kota].copy()\n",
    "        city_data = city_data.sort_values('tahun')\n",
    "        \n",
    "        if len(city_data) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Prepare data for linear regression\n",
    "        X = city_data['tahun'].values.reshape(-1, 1)\n",
    "        y = city_data['jumlah_penduduk'].values\n",
    "        \n",
    "        # Fit linear regression\n",
    "        model = LinearRegression()\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        # Get slope and intercept for understanding trend\n",
    "        slope = model.coef_[0]\n",
    "        intercept = model.intercept_\n",
    "        \n",
    "        # Predict for target years\n",
    "        for year in target_years:\n",
    "            predicted_pop = model.predict([[year]])[0]\n",
    "            # Ensure population is positive - use last known value if prediction is negative\n",
    "            if predicted_pop <= 0:\n",
    "                predicted_pop = city_data['jumlah_penduduk'].iloc[-1]\n",
    "            \n",
    "            extrapolated_data.append({\n",
    "                'tahun': year,\n",
    "                'nama_kabupaten_kota': kota,\n",
    "                'jumlah_penduduk': int(predicted_pop),\n",
    "                'annual_growth_rate': slope / city_data['jumlah_penduduk'].mean() * 100,\n",
    "                'is_extrapolated': True\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(extrapolated_data)\n",
    "\n",
    "# Perform population extrapolation at city level\n",
    "target_years = list(range(START_YEAR, END_YEAR + 1))\n",
    "df_pop_extrapolated = extrapolate_population_by_city(df_population, target_years)\n",
    "\n",
    "print(f\"✓ Population extrapolated for years: {target_years}\")\n",
    "print(f\"  Total records: {df_pop_extrapolated.shape[0]:,}\")\n",
    "print(f\"  Cities: {df_pop_extrapolated['nama_kabupaten_kota'].unique().tolist()}\")\n",
    "\n",
    "# Show extrapolation summary\n",
    "print(\"\\n  Extrapolated population by city:\")\n",
    "pivot_table = df_pop_extrapolated.pivot(index='nama_kabupaten_kota', columns='tahun', values='jumlah_penduduk')\n",
    "print(pivot_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ba16bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Population mapped to stations\n",
      "\n",
      "Extrapolated population by station (2022-2025):\n",
      "tahun          2022     2023     2024     2025\n",
      "stasiun_id                                    \n",
      "DKI1        1311376  1328565  1345755  1362945\n",
      "DKI2        1724833  1725866  1726898  1727931\n",
      "DKI3        2699172  2767771  2836369  2904968\n",
      "DKI4        2442570  2382158  2321747  2261335\n",
      "DKI5        2332364  2332364  2332364  2332364\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 7: Create Kecamatan-to-Station Mapping\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "MAPPING LOGIC:\n",
    "--------------\n",
    "Each ISPU monitoring station is located in a specific area of Jakarta.\n",
    "We map kabupaten/kota (city) to the nearest station based on administrative area:\n",
    "- DKI1 (Bundaran HI) -> Jakarta Pusat\n",
    "- DKI2 (Kelapa Gading) -> Jakarta Utara  \n",
    "- DKI3 (Jagakarsa) -> Jakarta Selatan\n",
    "- DKI4 (Lubang Buaya) -> Jakarta Timur\n",
    "- DKI5 (Kebon Jeruk) -> Jakarta Barat\n",
    "\"\"\"\n",
    "\n",
    "# Map kabupaten/kota to stations\n",
    "KOTA_TO_STATION = {\n",
    "    'JAKARTA PUSAT': 'DKI1',\n",
    "    'JAKARTA UTARA': 'DKI2',\n",
    "    'JAKARTA SELATAN': 'DKI3',\n",
    "    'JAKARTA TIMUR': 'DKI4',\n",
    "    'JAKARTA BARAT': 'DKI5',\n",
    "    'KAB.ADM.KEP.SERIBU': 'DKI2'  # Map to nearest (north)\n",
    "}\n",
    "\n",
    "def map_population_to_stations(df_pop, kota_station_map):\n",
    "    \"\"\"\n",
    "    Map population by station based on administrative area mapping.\n",
    "    \n",
    "    Parameters:\n",
    "        df_pop: Population DataFrame with nama_kabupaten_kota column\n",
    "        kota_station_map: Dictionary mapping kota names to station IDs\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with population mapped to station by year\n",
    "    \"\"\"\n",
    "    df = df_pop.copy()\n",
    "    \n",
    "    # Map kota to station\n",
    "    df['stasiun_id'] = df['nama_kabupaten_kota'].map(kota_station_map)\n",
    "    \n",
    "    # Aggregate population by station and year (in case multiple kota map to same station)\n",
    "    df_agg = df.groupby(['tahun', 'stasiun_id']).agg({\n",
    "        'jumlah_penduduk': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    return df_agg\n",
    "\n",
    "# Map extrapolated population to stations\n",
    "df_pop_by_station = map_population_to_stations(df_pop_extrapolated, KOTA_TO_STATION)\n",
    "\n",
    "print(\"✓ Population mapped to stations\")\n",
    "print(\"\\nExtrapolated population by station (2022-2025):\")\n",
    "pivot = df_pop_by_station.pivot(index='stasiun_id', columns='tahun', values='jumlah_penduduk')\n",
    "print(pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cebf4d",
   "metadata": {},
   "source": [
    "## Section 3: Spatial Mapping using Haversine Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f0a7072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Haversine distance function created\n",
      "  Test: Point (-6.286182, 106.870626) -> Nearest station: DKI4 (3.48 km)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 8: Haversine Distance Function for Spatial Mapping\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "HAVERSINE FORMULA:\n",
    "------------------\n",
    "The Haversine formula calculates the great-circle distance between two points\n",
    "on a sphere given their latitude and longitude coordinates.\n",
    "\n",
    "Formula:\n",
    "    a = sin²(Δlat/2) + cos(lat1) × cos(lat2) × sin²(Δlon/2)\n",
    "    c = 2 × atan2(√a, √(1-a))\n",
    "    d = R × c\n",
    "\n",
    "Where:\n",
    "    - R = Earth's radius (6371 km)\n",
    "    - Δlat = lat2 - lat1 (in radians)\n",
    "    - Δlon = lon2 - lon1 (in radians)\n",
    "\n",
    "This is used to find the nearest ISPU station for each river sampling point.\n",
    "\"\"\"\n",
    "\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calculate the Haversine distance between two points on Earth.\n",
    "    \n",
    "    Parameters:\n",
    "        lat1, lon1: Latitude and longitude of first point (in degrees)\n",
    "        lat2, lon2: Latitude and longitude of second point (in degrees)\n",
    "    \n",
    "    Returns:\n",
    "        Distance in kilometers\n",
    "    \"\"\"\n",
    "    # Earth's radius in kilometers\n",
    "    R = 6371.0\n",
    "    \n",
    "    # Convert degrees to radians\n",
    "    lat1_rad = np.radians(lat1)\n",
    "    lat2_rad = np.radians(lat2)\n",
    "    lon1_rad = np.radians(lon1)\n",
    "    lon2_rad = np.radians(lon2)\n",
    "    \n",
    "    # Differences\n",
    "    dlat = lat2_rad - lat1_rad\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "    \n",
    "    # Haversine formula\n",
    "    a = np.sin(dlat / 2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    \n",
    "    distance = R * c\n",
    "    return distance\n",
    "\n",
    "def find_nearest_station(lat, lon, station_coords):\n",
    "    \"\"\"\n",
    "    Find the nearest ISPU station to a given coordinate.\n",
    "    \n",
    "    Parameters:\n",
    "        lat, lon: Coordinates of the point\n",
    "        station_coords: Dictionary of station coordinates {station_id: {'lat': x, 'lon': y}}\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (station_id, distance_km)\n",
    "    \"\"\"\n",
    "    min_distance = float('inf')\n",
    "    nearest_station = None\n",
    "    \n",
    "    for station_id, coords in station_coords.items():\n",
    "        distance = haversine_distance(lat, lon, coords['lat'], coords['lon'])\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            nearest_station = station_id\n",
    "    \n",
    "    return nearest_station, min_distance\n",
    "\n",
    "# Test the function\n",
    "test_lat, test_lon = -6.286182, 106.870626  # Kalibaru Timur river sample point\n",
    "nearest, dist = find_nearest_station(test_lat, test_lon, STATION_COORDS)\n",
    "print(f\"✓ Haversine distance function created\")\n",
    "print(f\"  Test: Point ({test_lat}, {test_lon}) -> Nearest station: {nearest} ({dist:.2f} km)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5b8b7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ River quality data mapped to ISPU stations\n",
      "\n",
      "  Mapping summary:\n",
      "           titik_sampel distance_to_station_km             \n",
      "                nunique                   mean   min    max\n",
      "stasiun_id                                                 \n",
      "DKI1                 33                   4.31  1.13   8.04\n",
      "DKI2                 21                   5.23  2.40   9.04\n",
      "DKI3                 16                   4.13  0.52   7.91\n",
      "DKI4                 21                   5.13  1.10   7.31\n",
      "DKI5                 29                   5.75  2.11  11.36\n",
      "\n",
      "✓ River quality aggregated by station and period\n",
      "  Available parameters: ['pH', 'BOD', 'COD', 'DO', 'TSS']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>periode_data</th>\n",
       "      <th>stasiun_id</th>\n",
       "      <th>pH</th>\n",
       "      <th>BOD</th>\n",
       "      <th>COD</th>\n",
       "      <th>DO</th>\n",
       "      <th>TSS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024</td>\n",
       "      <td>DKI1</td>\n",
       "      <td>7.181212</td>\n",
       "      <td>19.480227</td>\n",
       "      <td>104.645152</td>\n",
       "      <td>2.034058</td>\n",
       "      <td>64.423030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024</td>\n",
       "      <td>DKI2</td>\n",
       "      <td>7.276426</td>\n",
       "      <td>29.529643</td>\n",
       "      <td>156.428571</td>\n",
       "      <td>1.811929</td>\n",
       "      <td>67.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024</td>\n",
       "      <td>DKI3</td>\n",
       "      <td>7.079150</td>\n",
       "      <td>12.824219</td>\n",
       "      <td>57.618594</td>\n",
       "      <td>3.410164</td>\n",
       "      <td>40.265625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024</td>\n",
       "      <td>DKI4</td>\n",
       "      <td>7.165417</td>\n",
       "      <td>17.854048</td>\n",
       "      <td>80.211190</td>\n",
       "      <td>2.802082</td>\n",
       "      <td>70.410714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024</td>\n",
       "      <td>DKI5</td>\n",
       "      <td>7.179496</td>\n",
       "      <td>16.237241</td>\n",
       "      <td>79.688879</td>\n",
       "      <td>2.075152</td>\n",
       "      <td>34.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   periode_data stasiun_id        pH        BOD         COD        DO  \\\n",
       "0          2024       DKI1  7.181212  19.480227  104.645152  2.034058   \n",
       "1          2024       DKI2  7.276426  29.529643  156.428571  1.811929   \n",
       "2          2024       DKI3  7.079150  12.824219   57.618594  3.410164   \n",
       "3          2024       DKI4  7.165417  17.854048   80.211190  2.802082   \n",
       "4          2024       DKI5  7.179496  16.237241   79.688879  2.075152   \n",
       "\n",
       "         TSS  \n",
       "0  64.423030  \n",
       "1  67.125000  \n",
       "2  40.265625  \n",
       "3  70.410714  \n",
       "4  34.750000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 9: Map River Quality Data to ISPU Stations\n",
    "# =============================================================================\n",
    "\n",
    "def map_river_to_stations(df_river, station_coords):\n",
    "    \"\"\"\n",
    "    Map each river sampling point to the nearest ISPU station using Haversine distance.\n",
    "    \n",
    "    Parameters:\n",
    "        df_river: River quality DataFrame with latitude/longitude columns\n",
    "        station_coords: Dictionary of station coordinates\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with river data mapped to nearest stations\n",
    "    \"\"\"\n",
    "    df = df_river.copy()\n",
    "    \n",
    "    # Find nearest station for each sampling point\n",
    "    station_assignments = []\n",
    "    distances = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        lat, lon = row['latitude'], row['longitude']\n",
    "        if pd.notna(lat) and pd.notna(lon):\n",
    "            station, dist = find_nearest_station(lat, lon, station_coords)\n",
    "            station_assignments.append(station)\n",
    "            distances.append(dist)\n",
    "        else:\n",
    "            station_assignments.append(None)\n",
    "            distances.append(None)\n",
    "    \n",
    "    df['stasiun_id'] = station_assignments\n",
    "    df['distance_to_station_km'] = distances\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Map river data to stations\n",
    "df_river_mapped = map_river_to_stations(df_river, STATION_COORDS)\n",
    "\n",
    "print(\"✓ River quality data mapped to ISPU stations\")\n",
    "print(f\"\\n  Mapping summary:\")\n",
    "print(df_river_mapped.groupby('stasiun_id').agg({\n",
    "    'titik_sampel': 'nunique',\n",
    "    'distance_to_station_km': ['mean', 'min', 'max']\n",
    "}).round(2))\n",
    "\n",
    "# Aggregate river quality by station and period\n",
    "# Select key water quality parameters\n",
    "key_params = ['pH', 'BOD', 'COD', 'DO', 'TSS']\n",
    "available_params = [p for p in key_params if p in df_river_mapped.columns]\n",
    "\n",
    "df_river_agg = df_river_mapped.groupby(['periode_data', 'stasiun_id']).agg({\n",
    "    **{param: 'mean' for param in available_params}\n",
    "}).reset_index()\n",
    "\n",
    "print(f\"\\n✓ River quality aggregated by station and period\")\n",
    "print(f\"  Available parameters: {available_params}\")\n",
    "df_river_agg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80eae25",
   "metadata": {},
   "source": [
    "## Section 4: Advanced Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce77f71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Wind direction encoded to circular components\n",
      "  Sample encoding (first 5 rows):\n",
      "        tanggal stasiun_id  wind_direction_10m_dominant  wind_sin  wind_cos\n",
      "4383 2022-01-01       DKI1                          259 -0.981627 -0.190809\n",
      "4384 2022-01-02       DKI1                          254 -0.961262 -0.275637\n",
      "4385 2022-01-03       DKI1                          231 -0.777146 -0.629320\n",
      "4386 2022-01-04       DKI1                          356 -0.069756  0.997564\n",
      "4387 2022-01-05       DKI1                          168  0.207912 -0.978148\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 10: Wind Direction Circular Encoding\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "CIRCULAR ENCODING FOR WIND DIRECTION:\n",
    "-------------------------------------\n",
    "Wind direction is a circular/cyclical variable where 0° and 360° are the same.\n",
    "Standard linear encoding doesn't capture this circularity.\n",
    "\n",
    "Solution: Convert to sine and cosine components:\n",
    "    Wind_sin = sin(direction × π/180)\n",
    "    Wind_cos = cos(direction × π/180)\n",
    "\n",
    "This transformation:\n",
    "1. Preserves the circular nature (0° = 360°)\n",
    "2. Creates continuous features that ML models can interpret\n",
    "3. Captures both the north-south (cos) and east-west (sin) components\n",
    "\n",
    "Example:\n",
    "- North (0°): sin=0, cos=1\n",
    "- East (90°): sin=1, cos=0\n",
    "- South (180°): sin=0, cos=-1\n",
    "- West (270°): sin=-1, cos=0\n",
    "\"\"\"\n",
    "\n",
    "def encode_wind_direction(df, wind_col='wind_direction_10m_dominant'):\n",
    "    \"\"\"\n",
    "    Convert wind direction (degrees) to sine and cosine components.\n",
    "    \n",
    "    Parameters:\n",
    "        df: DataFrame with wind direction column\n",
    "        wind_col: Name of the wind direction column (in degrees)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with wind_sin and wind_cos columns added\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    if wind_col in df.columns:\n",
    "        # Convert degrees to radians\n",
    "        direction_rad = df[wind_col] * np.pi / 180\n",
    "        \n",
    "        # Calculate sine and cosine components\n",
    "        df['wind_sin'] = np.sin(direction_rad)\n",
    "        df['wind_cos'] = np.cos(direction_rad)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply to weather data\n",
    "df_weather_encoded = encode_wind_direction(df_weather_filtered)\n",
    "\n",
    "print(\"✓ Wind direction encoded to circular components\")\n",
    "print(f\"  Sample encoding (first 5 rows):\")\n",
    "print(df_weather_encoded[['tanggal', 'stasiun_id', 'wind_direction_10m_dominant', 'wind_sin', 'wind_cos']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00e08fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Lag features created\n",
      "  Columns: ['pm_sepuluh', 'pm_duakomalima']\n",
      "  Lag periods: [1, 7] days\n",
      "\n",
      "  Sample lag features (DKI1, first 10 rows):\n",
      "        tanggal stasiun_id pm_sepuluh pm_sepuluh_lag_1d pm_sepuluh_lag_7d  \\\n",
      "1918 2022-09-13       DKI1         59               NaN               NaN   \n",
      "1919 2022-10-07       DKI1         51                59               NaN   \n",
      "1920 2022-10-18       DKI1         59                51               NaN   \n",
      "1921 2022-12-01       DKI1         54                59               NaN   \n",
      "1922 2022-12-02       DKI1         53                54               NaN   \n",
      "1923 2022-12-03       DKI1         60                53               NaN   \n",
      "1924 2022-12-04       DKI1         58                60               NaN   \n",
      "1925 2022-12-05       DKI1         54                58                59   \n",
      "1926 2022-12-06       DKI1         58                54                51   \n",
      "1927 2022-12-07       DKI1         48                58                59   \n",
      "\n",
      "     pm_duakomalima pm_duakomalima_lag_1d pm_duakomalima_lag_7d  \n",
      "1918             81                   NaN                   NaN  \n",
      "1919             66                    81                   NaN  \n",
      "1920             79                    66                   NaN  \n",
      "1921             73                    79                   NaN  \n",
      "1922             67                    73                   NaN  \n",
      "1923             76                    67                   NaN  \n",
      "1924             76                    76                   NaN  \n",
      "1925             74                    76                    81  \n",
      "1926             79                    74                    66  \n",
      "1927             68                    79                    79  \n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 11: Lag Features for Air Quality Data\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "LAG FEATURES:\n",
    "-------------\n",
    "Lag features capture temporal dependencies in air quality data.\n",
    "Air quality on a given day is often influenced by conditions on previous days.\n",
    "\n",
    "We create:\n",
    "- 1-day lag: Captures immediate short-term persistence\n",
    "- 7-day lag: Captures weekly patterns (e.g., weekday vs weekend effects)\n",
    "\n",
    "These are crucial for time-series forecasting as they provide the model\n",
    "with historical context about pollution levels.\n",
    "\"\"\"\n",
    "\n",
    "def create_lag_features(df, columns, lags, group_col='stasiun_id', date_col='tanggal'):\n",
    "    \"\"\"\n",
    "    Create lag features for specified columns.\n",
    "    \n",
    "    Parameters:\n",
    "        df: DataFrame with time series data\n",
    "        columns: List of columns to create lags for\n",
    "        lags: List of lag periods (in days)\n",
    "        group_col: Column to group by (for per-station lags)\n",
    "        date_col: Date column for sorting\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with lag features added\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df = df.sort_values([group_col, date_col])\n",
    "    \n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            for lag in lags:\n",
    "                lag_col_name = f\"{col}_lag_{lag}d\"\n",
    "                df[lag_col_name] = df.groupby(group_col)[col].shift(lag)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Define columns for lag features\n",
    "lag_columns = ['pm_sepuluh', 'pm_duakomalima']\n",
    "lag_periods = [1, 7]  # 1-day and 7-day lags\n",
    "\n",
    "# Apply lag features to ISPU data\n",
    "df_ispu_with_lags = create_lag_features(df_ispu_filtered, lag_columns, lag_periods)\n",
    "\n",
    "print(\"✓ Lag features created\")\n",
    "print(f\"  Columns: {lag_columns}\")\n",
    "print(f\"  Lag periods: {lag_periods} days\")\n",
    "print(f\"\\n  Sample lag features (DKI1, first 10 rows):\")\n",
    "lag_display_cols = ['tanggal', 'stasiun_id', 'pm_sepuluh', 'pm_sepuluh_lag_1d', 'pm_sepuluh_lag_7d', \n",
    "                    'pm_duakomalima', 'pm_duakomalima_lag_1d', 'pm_duakomalima_lag_7d']\n",
    "existing_cols = [c for c in lag_display_cols if c in df_ispu_with_lags.columns]\n",
    "print(df_ispu_with_lags[df_ispu_with_lags['stasiun_id'] == 'DKI1'][existing_cols].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d979ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Rolling window features created (3-day mean)\n",
      "  Columns: ['precipitation_sum', 'temp_mean']\n",
      "\n",
      "  Sample rolling features (DKI1, first 10 rows):\n",
      "        tanggal stasiun_id  precipitation_sum  \\\n",
      "4383 2022-01-01       DKI1                0.3   \n",
      "4384 2022-01-02       DKI1                1.2   \n",
      "4385 2022-01-03       DKI1                0.2   \n",
      "4386 2022-01-04       DKI1                0.1   \n",
      "4387 2022-01-05       DKI1               15.9   \n",
      "4388 2022-01-06       DKI1               11.4   \n",
      "4389 2022-01-07       DKI1                7.9   \n",
      "4390 2022-01-08       DKI1                3.4   \n",
      "4391 2022-01-09       DKI1                4.6   \n",
      "4392 2022-01-10       DKI1               16.1   \n",
      "\n",
      "      precipitation_sum_rolling_3d_mean  temp_mean  temp_mean_rolling_3d_mean  \n",
      "4383                           0.300000       27.1                  27.100000  \n",
      "4384                           0.750000       27.3                  27.200000  \n",
      "4385                           0.566667       27.4                  27.266667  \n",
      "4386                           0.500000       27.4                  27.366667  \n",
      "4387                           5.400000       26.5                  27.100000  \n",
      "4388                           9.133333       26.3                  26.733333  \n",
      "4389                          11.733333       27.1                  26.633333  \n",
      "4390                           7.566667       27.3                  26.900000  \n",
      "4391                           5.300000       26.6                  27.000000  \n",
      "4392                           8.033333       25.7                  26.533333  \n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 12: Rolling Window Features for Weather Data\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "ROLLING WINDOW FEATURES:\n",
    "------------------------\n",
    "Rolling window statistics smooth out daily variations and capture trends.\n",
    "A 3-day moving average helps:\n",
    "1. Reduce noise from day-to-day weather fluctuations\n",
    "2. Capture the cumulative effect of weather conditions\n",
    "3. Provide more stable features for model training\n",
    "\n",
    "We apply this to precipitation_sum and temperature_2m_mean.\n",
    "\"\"\"\n",
    "\n",
    "def create_rolling_features(df, columns, window_size, group_col='stasiun_id', date_col='tanggal'):\n",
    "    \"\"\"\n",
    "    Create rolling window features for specified columns.\n",
    "    \n",
    "    Parameters:\n",
    "        df: DataFrame with time series data\n",
    "        columns: List of columns to create rolling features for\n",
    "        window_size: Window size for rolling statistics\n",
    "        group_col: Column to group by\n",
    "        date_col: Date column for sorting\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with rolling features added\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df = df.sort_values([group_col, date_col])\n",
    "    \n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            # Create rolling mean\n",
    "            rolling_col_name = f\"{col}_rolling_{window_size}d_mean\"\n",
    "            df[rolling_col_name] = df.groupby(group_col)[col].transform(\n",
    "                lambda x: x.rolling(window=window_size, min_periods=1).mean()\n",
    "            )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Define columns for rolling features\n",
    "rolling_columns = ['precipitation_sum', 'temp_mean']\n",
    "window_size = 3  # 3-day moving average\n",
    "\n",
    "# Apply rolling features to weather data\n",
    "df_weather_with_rolling = create_rolling_features(df_weather_encoded, rolling_columns, window_size)\n",
    "\n",
    "print(f\"✓ Rolling window features created ({window_size}-day mean)\")\n",
    "print(f\"  Columns: {rolling_columns}\")\n",
    "print(f\"\\n  Sample rolling features (DKI1, first 10 rows):\")\n",
    "rolling_display_cols = ['tanggal', 'stasiun_id', 'precipitation_sum', 'precipitation_sum_rolling_3d_mean',\n",
    "                        'temp_mean', 'temp_mean_rolling_3d_mean']\n",
    "print(df_weather_with_rolling[df_weather_with_rolling['stasiun_id'] == 'DKI1'][rolling_display_cols].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a67db0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Time features extracted\n",
      "  Features: month, is_weekend, is_holiday_nasional\n",
      "\n",
      "  Sample time features (first 10 rows):\n",
      "     tanggal stasiun_id  month  is_weekend  is_holiday_nasional\n",
      "0 2022-01-01       DKI1      1           1                    1\n",
      "1 2022-01-02       DKI1      1           1                    0\n",
      "2 2022-01-03       DKI1      1           0                    0\n",
      "3 2022-01-04       DKI1      1           0                    0\n",
      "4 2022-01-05       DKI1      1           0                    0\n",
      "5 2022-01-06       DKI1      1           0                    0\n",
      "6 2022-01-07       DKI1      1           0                    0\n",
      "7 2022-01-08       DKI1      1           1                    0\n",
      "8 2022-01-09       DKI1      1           1                    0\n",
      "9 2022-01-10       DKI1      1           0                    0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 13: Time Features Extraction\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "TIME FEATURES:\n",
    "--------------\n",
    "Extract temporal features that capture seasonal and weekly patterns:\n",
    "1. month: Captures seasonal patterns (rainy/dry season in Jakarta)\n",
    "2. is_weekend: Weekend traffic patterns differ from weekdays\n",
    "3. is_holiday_nasional: National holidays affect traffic and industrial activity\n",
    "\"\"\"\n",
    "\n",
    "def extract_time_features(df, df_holidays, date_col='tanggal'):\n",
    "    \"\"\"\n",
    "    Extract time-based features from date column.\n",
    "    \n",
    "    Parameters:\n",
    "        df: DataFrame with date column\n",
    "        df_holidays: Holiday DataFrame with is_weekend and is_holiday_nasional\n",
    "        date_col: Name of date column\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with time features added\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Extract month\n",
    "    df['month'] = df[date_col].dt.month\n",
    "    \n",
    "    # Merge with holiday data\n",
    "    holiday_cols = [date_col, 'is_weekend', 'is_holiday_nasional']\n",
    "    df_holidays_subset = df_holidays[[c for c in holiday_cols if c in df_holidays.columns]].copy()\n",
    "    \n",
    "    # Merge holidays\n",
    "    df = df.merge(df_holidays_subset, on=date_col, how='left')\n",
    "    \n",
    "    # Fill missing values (in case some dates don't have holiday data)\n",
    "    if 'is_weekend' in df.columns:\n",
    "        df['is_weekend'] = df['is_weekend'].fillna(0).astype(int)\n",
    "    else:\n",
    "        df['is_weekend'] = df[date_col].dt.dayofweek.isin([5, 6]).astype(int)\n",
    "    \n",
    "    if 'is_holiday_nasional' in df.columns:\n",
    "        df['is_holiday_nasional'] = df['is_holiday_nasional'].fillna(0).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply time features to weather data (main dataframe)\n",
    "df_weather_with_time = extract_time_features(df_weather_with_rolling, df_holidays_filtered)\n",
    "\n",
    "print(\"✓ Time features extracted\")\n",
    "print(f\"  Features: month, is_weekend, is_holiday_nasional\")\n",
    "print(f\"\\n  Sample time features (first 10 rows):\")\n",
    "time_display_cols = ['tanggal', 'stasiun_id', 'month', 'is_weekend', 'is_holiday_nasional']\n",
    "print(df_weather_with_time[time_display_cols].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8537add5",
   "metadata": {},
   "source": [
    "## Section 5: Data Cleaning & Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d17cb599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values BEFORE interpolation:\n",
      "  pm_sepuluh: 186 (3.6%)\n",
      "  pm_duakomalima: 53 (1.0%)\n",
      "  sulfur_dioksida: 53 (1.0%)\n",
      "  karbon_monoksida: 44 (0.9%)\n",
      "  ozon: 43 (0.8%)\n",
      "  nitrogen_dioksida: 66 (1.3%)\n",
      "  max: 7 (0.1%)\n",
      "\n",
      "Missing values AFTER interpolation:\n",
      "  pm_sepuluh: 1 (0.0%)\n",
      "  pm_duakomalima: 1 (0.0%)\n",
      "  sulfur_dioksida: 1 (0.0%)\n",
      "  karbon_monoksida: 1 (0.0%)\n",
      "  ozon: 1 (0.0%)\n",
      "  nitrogen_dioksida: 1 (0.0%)\n",
      "  max: 1 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 14: Handle Missing Values with Time-Series Interpolation\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "MISSING VALUE HANDLING:\n",
    "-----------------------\n",
    "For pollutant columns, we use linear interpolation which:\n",
    "1. Estimates missing values based on surrounding time points\n",
    "2. Preserves temporal continuity in the data\n",
    "3. Is more appropriate than mean imputation for time-series data\n",
    "\n",
    "Linear interpolation: value = prev_value + (next_value - prev_value) * ratio\n",
    "\"\"\"\n",
    "\n",
    "def interpolate_pollutants(df, pollutant_cols, group_col='stasiun_id', date_col='tanggal'):\n",
    "    \"\"\"\n",
    "    Handle missing values in pollutant columns using time-series linear interpolation.\n",
    "    \n",
    "    Parameters:\n",
    "        df: DataFrame with pollutant columns\n",
    "        pollutant_cols: List of pollutant column names\n",
    "        group_col: Column to group by for interpolation\n",
    "        date_col: Date column for sorting\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with interpolated values\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df = df.sort_values([group_col, date_col])\n",
    "    \n",
    "    # First convert pollutant columns to numeric, handling string missing values like '-', '---'\n",
    "    for col in pollutant_cols:\n",
    "        if col in df.columns:\n",
    "            # Convert to string first, then handle non-numeric values\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # Interpolate within each station group\n",
    "    for col in pollutant_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df.groupby(group_col)[col].transform(\n",
    "                lambda x: x.interpolate(method='linear', limit_direction='both')\n",
    "            )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Define pollutant columns\n",
    "pollutant_columns = ['pm_sepuluh', 'pm_duakomalima', 'sulfur_dioksida', \n",
    "                     'karbon_monoksida', 'ozon', 'nitrogen_dioksida', 'max']\n",
    "\n",
    "# Check missing values before interpolation\n",
    "print(\"Missing values BEFORE interpolation:\")\n",
    "for col in pollutant_columns:\n",
    "    if col in df_ispu_with_lags.columns:\n",
    "        missing = df_ispu_with_lags[col].isna().sum()\n",
    "        total = len(df_ispu_with_lags)\n",
    "        print(f\"  {col}: {missing:,} ({missing/total*100:.1f}%)\")\n",
    "\n",
    "# Apply interpolation\n",
    "df_ispu_interpolated = interpolate_pollutants(df_ispu_with_lags, pollutant_columns)\n",
    "\n",
    "print(\"\\nMissing values AFTER interpolation:\")\n",
    "for col in pollutant_columns:\n",
    "    if col in df_ispu_interpolated.columns:\n",
    "        missing = df_ispu_interpolated[col].isna().sum()\n",
    "        total = len(df_ispu_interpolated)\n",
    "        print(f\"  {col}: {missing:,} ({missing/total*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfd1dd81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Target variable encoded\n",
      "\n",
      "  Encoding mapping:\n",
      "    BAIK: 0\n",
      "    SEDANG: 1\n",
      "    TIDAK SEHAT: 2\n",
      "    SANGAT TIDAK SEHAT: 3\n",
      "    BERBAHAYA: 4\n",
      "\n",
      "  Encoded value distribution:\n",
      "kategori_encoded\n",
      "-1.0      39\n",
      " 0.0     618\n",
      " 1.0    3864\n",
      " 2.0     651\n",
      " 3.0       4\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 15: Ordinal Encoding for Target Variable (kategori)\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "ORDINAL ENCODING:\n",
    "-----------------\n",
    "The target variable 'kategori' represents air quality levels with a natural order:\n",
    "- BAIK (Good): 0\n",
    "- SEDANG (Moderate): 1\n",
    "- TIDAK SEHAT (Unhealthy): 2\n",
    "- SANGAT TIDAK SEHAT (Very Unhealthy): 3\n",
    "- BERBAHAYA (Hazardous): 4\n",
    "\n",
    "We use OrdinalEncoder to preserve this natural ordering, which is important\n",
    "for ordinal regression or when treating this as a regression problem.\n",
    "\"\"\"\n",
    "\n",
    "def encode_target_variable(df, target_col='kategori'):\n",
    "    \"\"\"\n",
    "    Encode target variable using ordinal encoding.\n",
    "    \n",
    "    Parameters:\n",
    "        df: DataFrame with target column\n",
    "        target_col: Name of target column\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (DataFrame with encoded target, encoder object)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Define category order (from best to worst air quality)\n",
    "    category_order = ['BAIK', 'SEDANG', 'TIDAK SEHAT', 'SANGAT TIDAK SEHAT', 'BERBAHAYA']\n",
    "    \n",
    "    # Standardize category names (uppercase, strip whitespace)\n",
    "    if target_col in df.columns:\n",
    "        df[target_col] = df[target_col].str.upper().str.strip()\n",
    "    \n",
    "    # Create ordinal encoder\n",
    "    encoder = OrdinalEncoder(categories=[category_order], handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "    \n",
    "    # Fit and transform\n",
    "    if target_col in df.columns:\n",
    "        # Handle NaN values\n",
    "        mask = df[target_col].notna()\n",
    "        df.loc[mask, 'kategori_encoded'] = encoder.fit_transform(\n",
    "            df.loc[mask, [target_col]]\n",
    "        ).flatten()\n",
    "        \n",
    "        # Fill NaN in encoded column\n",
    "        df['kategori_encoded'] = df['kategori_encoded'].fillna(-1)\n",
    "    \n",
    "    return df, encoder\n",
    "\n",
    "# Apply encoding\n",
    "df_ispu_encoded, target_encoder = encode_target_variable(df_ispu_interpolated)\n",
    "\n",
    "print(\"✓ Target variable encoded\")\n",
    "print(\"\\n  Encoding mapping:\")\n",
    "for i, cat in enumerate(['BAIK', 'SEDANG', 'TIDAK SEHAT', 'SANGAT TIDAK SEHAT', 'BERBAHAYA']):\n",
    "    print(f\"    {cat}: {i}\")\n",
    "\n",
    "print(\"\\n  Encoded value distribution:\")\n",
    "print(df_ispu_encoded['kategori_encoded'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b44aaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Outliers handled using IQR method\n",
      "\n",
      "  Outlier summary:\n",
      "    temp_max: 248 outliers (low: 90, high: 158)\n",
      "      Bounds: [27.25, 35.65]\n",
      "    temp_min: 58 outliers (low: 46, high: 12)\n",
      "      Bounds: [21.20, 26.00]\n",
      "    temp_mean: 41 outliers (low: 15, high: 26)\n",
      "      Bounds: [24.10, 29.70]\n",
      "    precipitation_sum: 245 outliers (low: 0, high: 245)\n",
      "      Bounds: [-13.45, 25.35]\n",
      "    wind_speed_max: 85 outliers (low: 0, high: 85)\n",
      "      Bounds: [3.10, 22.30]\n",
      "    wind_speed_mean: 384 outliers (low: 0, high: 384)\n",
      "      Bounds: [1.05, 11.05]\n",
      "    humidity_mean: 261 outliers (low: 261, high: 0)\n",
      "      Bounds: [66.00, 98.00]\n",
      "    pressure_mean: 7 outliers (low: 7, high: 0)\n",
      "      Bounds: [998.60, 1015.40]\n",
      "    cloud_cover_mean: 175 outliers (low: 175, high: 0)\n",
      "      Bounds: [25.50, 141.50]\n",
      "    radiation_sum: 231 outliers (low: 231, high: 0)\n",
      "      Bounds: [10.86, 27.38]\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 16: Handle Outliers using IQR Method for Weather Data\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "IQR (INTERQUARTILE RANGE) METHOD:\n",
    "---------------------------------\n",
    "The IQR method identifies outliers as values that fall outside:\n",
    "    Lower bound = Q1 - 1.5 × IQR\n",
    "    Upper bound = Q3 + 1.5 × IQR\n",
    "\n",
    "Where:\n",
    "    Q1 = 25th percentile (first quartile)\n",
    "    Q3 = 75th percentile (third quartile)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "Values outside these bounds are clipped (capped) to the bounds rather than removed,\n",
    "preserving the temporal continuity of the time series.\n",
    "\"\"\"\n",
    "\n",
    "def handle_outliers_iqr(df, columns, multiplier=1.5):\n",
    "    \"\"\"\n",
    "    Handle outliers using IQR method by clipping values.\n",
    "    \n",
    "    Parameters:\n",
    "        df: DataFrame with columns to process\n",
    "        columns: List of columns to check for outliers\n",
    "        multiplier: IQR multiplier for bounds (default 1.5)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with outliers clipped\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    outlier_summary = {}\n",
    "    \n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            # Calculate Q1, Q3, and IQR\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            \n",
    "            # Define bounds\n",
    "            lower_bound = Q1 - multiplier * IQR\n",
    "            upper_bound = Q3 + multiplier * IQR\n",
    "            \n",
    "            # Count outliers before clipping\n",
    "            outliers_low = (df[col] < lower_bound).sum()\n",
    "            outliers_high = (df[col] > upper_bound).sum()\n",
    "            \n",
    "            # Clip values\n",
    "            df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "            \n",
    "            outlier_summary[col] = {\n",
    "                'Q1': Q1, 'Q3': Q3, 'IQR': IQR,\n",
    "                'lower_bound': lower_bound, 'upper_bound': upper_bound,\n",
    "                'outliers_low': outliers_low, 'outliers_high': outliers_high\n",
    "            }\n",
    "    \n",
    "    return df, outlier_summary\n",
    "\n",
    "# Define weather columns to check for outliers\n",
    "weather_numeric_cols = ['temp_max', 'temp_min', 'temp_mean', 'precipitation_sum', \n",
    "                        'wind_speed_max', 'wind_speed_mean', 'humidity_mean',\n",
    "                        'pressure_mean', 'cloud_cover_mean', 'radiation_sum']\n",
    "\n",
    "# Apply IQR outlier handling\n",
    "df_weather_clean, outlier_info = handle_outliers_iqr(df_weather_with_time, weather_numeric_cols)\n",
    "\n",
    "print(\"✓ Outliers handled using IQR method\")\n",
    "print(\"\\n  Outlier summary:\")\n",
    "for col, info in outlier_info.items():\n",
    "    total_outliers = info['outliers_low'] + info['outliers_high']\n",
    "    if total_outliers > 0:\n",
    "        print(f\"    {col}: {total_outliers} outliers (low: {info['outliers_low']}, high: {info['outliers_high']})\")\n",
    "        print(f\"      Bounds: [{info['lower_bound']:.2f}, {info['upper_bound']:.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776dd201",
   "metadata": {},
   "source": [
    "## Section 6: Final Output - Merge All Features & Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "add6b1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Master dataframe created\n",
      "  Shape: (5175, 55)\n",
      "  Date range: 2022-01-01 to 2025-08-31\n",
      "  Stations: ['DKI1', 'DKI2', 'DKI3', 'DKI4', 'DKI5']\n",
      "\n",
      "  Columns (55):\n",
      "  ['tanggal', 'stasiun_id', 'stasiun', 'pm_sepuluh', 'pm_duakomalima', 'sulfur_dioksida', 'karbon_monoksida', 'ozon', 'nitrogen_dioksida', 'max', 'parameter_pencemar_kritis', 'kategori', 'year', 'pm_sepuluh_lag_1d', 'pm_sepuluh_lag_7d', 'pm_duakomalima_lag_1d', 'pm_duakomalima_lag_7d', 'kategori_encoded', 'temp_max', 'temp_min', 'precipitation_sum', 'precipitation_hours', 'wind_speed_max', 'wind_direction_10m_dominant', 'radiation_sum', 'temp_mean', 'humidity_mean', 'cloud_cover_mean', 'pressure_mean', 'wind_gusts_max', 'wind_direction_alt', 'humidity_max', 'humidity_min', 'cloud_cover_max', 'cloud_cover_min', 'wind_gusts_mean', 'wind_speed_mean', 'wind_gusts_min', 'wind_speed_min', 'pressure_max', 'pressure_min', 'wind_sin', 'wind_cos', 'precipitation_sum_rolling_3d_mean', 'temp_mean_rolling_3d_mean', 'month', 'is_weekend', 'is_holiday_nasional', 'jumlah_penduduk', 'ndvi', 'pH', 'BOD', 'COD', 'DO', 'TSS']\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 17: Merge All Features into Master DataFrame\n",
    "# =============================================================================\n",
    "\n",
    "def create_master_dataframe(df_ispu, df_weather, df_pop_by_station, df_ndvi, df_river_agg):\n",
    "    \"\"\"\n",
    "    Merge all processed dataframes into a single master dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "        df_ispu: Processed ISPU data with lag features\n",
    "        df_weather: Processed weather data with rolling features\n",
    "        df_pop_by_station: Population data by station\n",
    "        df_ndvi: NDVI vegetation index data\n",
    "        df_river_agg: Aggregated river quality data\n",
    "    \n",
    "    Returns:\n",
    "        Master DataFrame with all features\n",
    "    \"\"\"\n",
    "    # Start with ISPU data as base\n",
    "    master_df = df_ispu.copy()\n",
    "    \n",
    "    # Merge weather data\n",
    "    weather_cols_to_merge = [col for col in df_weather.columns \n",
    "                            if col not in ['year'] or col in ['tanggal', 'stasiun_id']]\n",
    "    master_df = master_df.merge(\n",
    "        df_weather[weather_cols_to_merge],\n",
    "        on=['tanggal', 'stasiun_id'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Add year column for population merge\n",
    "    master_df['year'] = master_df['tanggal'].dt.year\n",
    "    \n",
    "    # Merge population data\n",
    "    df_pop_by_station = df_pop_by_station.rename(columns={'tahun': 'year'})\n",
    "    master_df = master_df.merge(\n",
    "        df_pop_by_station[['year', 'stasiun_id', 'jumlah_penduduk']],\n",
    "        on=['year', 'stasiun_id'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Merge NDVI data (forward fill for dates between measurements)\n",
    "    df_ndvi_clean = df_ndvi[['tanggal', 'stasiun_id', 'ndvi']].copy()\n",
    "    df_ndvi_clean = df_ndvi_clean.rename(columns={'stasiun_id': 'ndvi_station'})\n",
    "    # Extract station ID from NDVI data\n",
    "    df_ndvi_clean['stasiun_id'] = df_ndvi_clean['ndvi_station']\n",
    "    df_ndvi_clean = df_ndvi_clean.drop(columns=['ndvi_station'])\n",
    "    \n",
    "    # For NDVI, we do an asof merge to get the most recent NDVI value\n",
    "    master_df = master_df.sort_values(['stasiun_id', 'tanggal'])\n",
    "    df_ndvi_clean = df_ndvi_clean.sort_values(['stasiun_id', 'tanggal'])\n",
    "    \n",
    "    # Merge NDVI using merge_asof for each station\n",
    "    master_dfs = []\n",
    "    for station in master_df['stasiun_id'].unique():\n",
    "        station_df = master_df[master_df['stasiun_id'] == station].copy()\n",
    "        station_ndvi = df_ndvi_clean[df_ndvi_clean['stasiun_id'] == station].copy()\n",
    "        \n",
    "        if len(station_ndvi) > 0:\n",
    "            merged = pd.merge_asof(\n",
    "                station_df.sort_values('tanggal'),\n",
    "                station_ndvi[['tanggal', 'ndvi']].sort_values('tanggal'),\n",
    "                on='tanggal',\n",
    "                direction='backward'\n",
    "            )\n",
    "            master_dfs.append(merged)\n",
    "        else:\n",
    "            station_df['ndvi'] = np.nan\n",
    "            master_dfs.append(station_df)\n",
    "    \n",
    "    master_df = pd.concat(master_dfs, ignore_index=True)\n",
    "    \n",
    "    # Merge river quality data (use yearly average)\n",
    "    df_river_yearly = df_river_agg.copy()\n",
    "    df_river_yearly = df_river_yearly.rename(columns={'periode_data': 'year'})\n",
    "    river_cols = ['year', 'stasiun_id', 'pH', 'BOD', 'COD', 'DO', 'TSS']\n",
    "    existing_river_cols = [c for c in river_cols if c in df_river_yearly.columns]\n",
    "    \n",
    "    master_df = master_df.merge(\n",
    "        df_river_yearly[existing_river_cols],\n",
    "        on=['year', 'stasiun_id'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    return master_df\n",
    "\n",
    "# Create master dataframe\n",
    "df_master = create_master_dataframe(\n",
    "    df_ispu_encoded,\n",
    "    df_weather_clean,\n",
    "    df_pop_by_station,\n",
    "    df_ndvi,\n",
    "    df_river_agg\n",
    ")\n",
    "\n",
    "print(f\"✓ Master dataframe created\")\n",
    "print(f\"  Shape: {df_master.shape}\")\n",
    "print(f\"  Date range: {df_master['tanggal'].min().date()} to {df_master['tanggal'].max().date()}\")\n",
    "print(f\"  Stations: {sorted(df_master['stasiun_id'].unique())}\")\n",
    "print(f\"\\n  Columns ({len(df_master.columns)}):\")\n",
    "print(f\"  {list(df_master.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c17bbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Master dataframe indexed by (tanggal, stasiun)\n",
      "✓ StandardScaler applied to 41 numeric features\n",
      "\n",
      "  Scaled columns:\n",
      "  ['pm_sepuluh', 'pm_duakomalima', 'sulfur_dioksida', 'karbon_monoksida', 'ozon', 'nitrogen_dioksida', 'max', 'temp_max', 'temp_min', 'precipitation_sum', 'precipitation_hours', 'wind_speed_max', 'wind_direction_10m_dominant', 'radiation_sum', 'temp_mean', 'humidity_mean', 'cloud_cover_mean', 'pressure_mean', 'wind_gusts_max', 'wind_direction_alt', 'humidity_max', 'humidity_min', 'cloud_cover_max', 'cloud_cover_min', 'wind_gusts_mean', 'wind_speed_mean', 'wind_gusts_min', 'wind_speed_min', 'pressure_max', 'pressure_min', 'wind_sin', 'wind_cos', 'precipitation_sum_rolling_3d_mean', 'temp_mean_rolling_3d_mean', 'jumlah_penduduk', 'ndvi', 'pH', 'BOD', 'COD', 'DO', 'TSS']\n",
      "\n",
      "  Final master dataframe shape: (5175, 53)\n",
      "\n",
      "  Sample of scaled data (first 5 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>stasiun</th>\n",
       "      <th>pm_sepuluh</th>\n",
       "      <th>pm_duakomalima</th>\n",
       "      <th>sulfur_dioksida</th>\n",
       "      <th>karbon_monoksida</th>\n",
       "      <th>ozon</th>\n",
       "      <th>nitrogen_dioksida</th>\n",
       "      <th>max</th>\n",
       "      <th>parameter_pencemar_kritis</th>\n",
       "      <th>kategori</th>\n",
       "      <th>...</th>\n",
       "      <th>month</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>is_holiday_nasional</th>\n",
       "      <th>jumlah_penduduk</th>\n",
       "      <th>ndvi</th>\n",
       "      <th>pH</th>\n",
       "      <th>BOD</th>\n",
       "      <th>COD</th>\n",
       "      <th>DO</th>\n",
       "      <th>TSS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tanggal</th>\n",
       "      <th>stasiun_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-01-01</th>\n",
       "      <th>DKI4</th>\n",
       "      <td>DKI4</td>\n",
       "      <td>0.315991</td>\n",
       "      <td>0.423425</td>\n",
       "      <td>0.707544</td>\n",
       "      <td>-0.219157</td>\n",
       "      <td>0.385961</td>\n",
       "      <td>-0.370514</td>\n",
       "      <td>0.432590</td>\n",
       "      <td>PM2,5</td>\n",
       "      <td>SEDANG</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.616397</td>\n",
       "      <td>1.548161</td>\n",
       "      <td>0.029933</td>\n",
       "      <td>-0.138312</td>\n",
       "      <td>-0.255926</td>\n",
       "      <td>-0.317809</td>\n",
       "      <td>0.325154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-02</th>\n",
       "      <th>DKI4</th>\n",
       "      <td>DKI4</td>\n",
       "      <td>-0.681598</td>\n",
       "      <td>-0.204145</td>\n",
       "      <td>0.510553</td>\n",
       "      <td>-0.652943</td>\n",
       "      <td>0.508530</td>\n",
       "      <td>-0.843441</td>\n",
       "      <td>-0.202411</td>\n",
       "      <td>PM2,5</td>\n",
       "      <td>SEDANG</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.616397</td>\n",
       "      <td>1.548161</td>\n",
       "      <td>0.029933</td>\n",
       "      <td>-0.138312</td>\n",
       "      <td>-0.255926</td>\n",
       "      <td>-0.317809</td>\n",
       "      <td>0.325154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-03</th>\n",
       "      <th>DKI4</th>\n",
       "      <td>DKI4</td>\n",
       "      <td>-0.153463</td>\n",
       "      <td>0.214235</td>\n",
       "      <td>0.773208</td>\n",
       "      <td>-0.219157</td>\n",
       "      <td>0.569814</td>\n",
       "      <td>-0.167832</td>\n",
       "      <td>0.220923</td>\n",
       "      <td>PM2,5</td>\n",
       "      <td>SEDANG</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.616397</td>\n",
       "      <td>1.548161</td>\n",
       "      <td>0.029933</td>\n",
       "      <td>-0.138312</td>\n",
       "      <td>-0.255926</td>\n",
       "      <td>-0.317809</td>\n",
       "      <td>0.325154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-04</th>\n",
       "      <th>DKI4</th>\n",
       "      <td>DKI4</td>\n",
       "      <td>0.433354</td>\n",
       "      <td>1.134671</td>\n",
       "      <td>0.904535</td>\n",
       "      <td>-0.074561</td>\n",
       "      <td>1.427794</td>\n",
       "      <td>0.169973</td>\n",
       "      <td>1.152258</td>\n",
       "      <td>PM2,5</td>\n",
       "      <td>TIDAK SEHAT</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.616397</td>\n",
       "      <td>1.548161</td>\n",
       "      <td>0.029933</td>\n",
       "      <td>-0.138312</td>\n",
       "      <td>-0.255926</td>\n",
       "      <td>-0.317809</td>\n",
       "      <td>0.325154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-05</th>\n",
       "      <th>DKI4</th>\n",
       "      <td>DKI4</td>\n",
       "      <td>2.545895</td>\n",
       "      <td>3.770465</td>\n",
       "      <td>1.035863</td>\n",
       "      <td>0.503821</td>\n",
       "      <td>1.672931</td>\n",
       "      <td>0.575338</td>\n",
       "      <td>3.819262</td>\n",
       "      <td>PM2,5</td>\n",
       "      <td>TIDAK SEHAT</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.616397</td>\n",
       "      <td>1.548161</td>\n",
       "      <td>0.029933</td>\n",
       "      <td>-0.138312</td>\n",
       "      <td>-0.255926</td>\n",
       "      <td>-0.317809</td>\n",
       "      <td>0.325154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      stasiun  pm_sepuluh  pm_duakomalima  sulfur_dioksida  \\\n",
       "tanggal    stasiun_id                                                        \n",
       "2022-01-01 DKI4          DKI4    0.315991        0.423425         0.707544   \n",
       "2022-01-02 DKI4          DKI4   -0.681598       -0.204145         0.510553   \n",
       "2022-01-03 DKI4          DKI4   -0.153463        0.214235         0.773208   \n",
       "2022-01-04 DKI4          DKI4    0.433354        1.134671         0.904535   \n",
       "2022-01-05 DKI4          DKI4    2.545895        3.770465         1.035863   \n",
       "\n",
       "                       karbon_monoksida      ozon  nitrogen_dioksida  \\\n",
       "tanggal    stasiun_id                                                  \n",
       "2022-01-01 DKI4               -0.219157  0.385961          -0.370514   \n",
       "2022-01-02 DKI4               -0.652943  0.508530          -0.843441   \n",
       "2022-01-03 DKI4               -0.219157  0.569814          -0.167832   \n",
       "2022-01-04 DKI4               -0.074561  1.427794           0.169973   \n",
       "2022-01-05 DKI4                0.503821  1.672931           0.575338   \n",
       "\n",
       "                            max parameter_pencemar_kritis     kategori  ...  \\\n",
       "tanggal    stasiun_id                                                   ...   \n",
       "2022-01-01 DKI4        0.432590                     PM2,5       SEDANG  ...   \n",
       "2022-01-02 DKI4       -0.202411                     PM2,5       SEDANG  ...   \n",
       "2022-01-03 DKI4        0.220923                     PM2,5       SEDANG  ...   \n",
       "2022-01-04 DKI4        1.152258                     PM2,5  TIDAK SEHAT  ...   \n",
       "2022-01-05 DKI4        3.819262                     PM2,5  TIDAK SEHAT  ...   \n",
       "\n",
       "                       month is_weekend is_holiday_nasional jumlah_penduduk  \\\n",
       "tanggal    stasiun_id                                                         \n",
       "2022-01-01 DKI4          1.0        1.0                 1.0        0.616397   \n",
       "2022-01-02 DKI4          1.0        1.0                 0.0        0.616397   \n",
       "2022-01-03 DKI4          1.0        0.0                 0.0        0.616397   \n",
       "2022-01-04 DKI4          1.0        0.0                 0.0        0.616397   \n",
       "2022-01-05 DKI4          1.0        0.0                 0.0        0.616397   \n",
       "\n",
       "                           ndvi        pH       BOD       COD        DO  \\\n",
       "tanggal    stasiun_id                                                     \n",
       "2022-01-01 DKI4        1.548161  0.029933 -0.138312 -0.255926 -0.317809   \n",
       "2022-01-02 DKI4        1.548161  0.029933 -0.138312 -0.255926 -0.317809   \n",
       "2022-01-03 DKI4        1.548161  0.029933 -0.138312 -0.255926 -0.317809   \n",
       "2022-01-04 DKI4        1.548161  0.029933 -0.138312 -0.255926 -0.317809   \n",
       "2022-01-05 DKI4        1.548161  0.029933 -0.138312 -0.255926 -0.317809   \n",
       "\n",
       "                            TSS  \n",
       "tanggal    stasiun_id            \n",
       "2022-01-01 DKI4        0.325154  \n",
       "2022-01-02 DKI4        0.325154  \n",
       "2022-01-03 DKI4        0.325154  \n",
       "2022-01-04 DKI4        0.325154  \n",
       "2022-01-05 DKI4        0.325154  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 18: Set Index and Apply StandardScaler\n",
    "# =============================================================================\n",
    "\n",
    "def scale_numeric_features(df, exclude_cols=None):\n",
    "    \"\"\"\n",
    "    Apply StandardScaler to all numeric features.\n",
    "    \n",
    "    StandardScaler transforms features to have:\n",
    "    - Mean = 0\n",
    "    - Standard deviation = 1\n",
    "    \n",
    "    Formula: z = (x - μ) / σ\n",
    "    \n",
    "    Parameters:\n",
    "        df: DataFrame with features\n",
    "        exclude_cols: Columns to exclude from scaling\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (scaled DataFrame, scaler object, list of scaled columns)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    if exclude_cols is None:\n",
    "        exclude_cols = []\n",
    "    \n",
    "    # Add default columns to exclude (identifiers, dates, categorical)\n",
    "    default_exclude = ['tanggal', 'stasiun_id', 'stasiun', 'kategori', 'kategori_encoded',\n",
    "                       'parameter_pencemar_kritis', 'year', 'is_weekend', 'is_holiday_nasional', 'month']\n",
    "    exclude_cols = list(set(exclude_cols + default_exclude))\n",
    "    \n",
    "    # Get numeric columns to scale\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    cols_to_scale = [col for col in numeric_cols if col not in exclude_cols]\n",
    "    \n",
    "    # Apply StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Handle missing values before scaling\n",
    "    df_to_scale = df[cols_to_scale].fillna(df[cols_to_scale].median())\n",
    "    \n",
    "    # Fit and transform\n",
    "    scaled_values = scaler.fit_transform(df_to_scale)\n",
    "    \n",
    "    # Create scaled dataframe\n",
    "    df_scaled = df.copy()\n",
    "    for i, col in enumerate(cols_to_scale):\n",
    "        df_scaled[col] = scaled_values[:, i]\n",
    "    \n",
    "    return df_scaled, scaler, cols_to_scale\n",
    "\n",
    "# Set multi-index on tanggal and stasiun\n",
    "df_master_indexed = df_master.set_index(['tanggal', 'stasiun_id']).sort_index()\n",
    "\n",
    "# Apply StandardScaler\n",
    "df_master_scaled, scaler, scaled_columns = scale_numeric_features(df_master)\n",
    "\n",
    "# Set index for scaled version too\n",
    "df_master_scaled_indexed = df_master_scaled.set_index(['tanggal', 'stasiun_id']).sort_index()\n",
    "\n",
    "print(\"✓ Master dataframe indexed by (tanggal, stasiun)\")\n",
    "print(f\"✓ StandardScaler applied to {len(scaled_columns)} numeric features\")\n",
    "print(f\"\\n  Scaled columns:\")\n",
    "print(f\"  {scaled_columns}\")\n",
    "\n",
    "print(f\"\\n  Final master dataframe shape: {df_master_scaled_indexed.shape}\")\n",
    "print(f\"\\n  Sample of scaled data (first 5 rows):\")\n",
    "df_master_scaled_indexed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19bdc6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DATA QUALITY REPORT - MASTER DATAFRAME\n",
      "======================================================================\n",
      "\n",
      "📊 BASIC INFO:\n",
      "   • Total records: 5,175\n",
      "   • Total features: 53\n",
      "   • Memory usage: 3.63 MB\n",
      "\n",
      "📅 TEMPORAL COVERAGE:\n",
      "   • Date range: 2022-01-01 to 2025-08-31\n",
      "   • Total days: 1339\n",
      "\n",
      "🏢 STATION COVERAGE:\n",
      "   • DKI1: 977 records\n",
      "   • DKI2: 1,005 records\n",
      "   • DKI3: 996 records\n",
      "   • DKI4: 1,207 records\n",
      "   • DKI5: 990 records\n",
      "\n",
      "⚠️ MISSING VALUES:\n",
      "   • No missing values in numeric columns!\n",
      "\n",
      "🎯 TARGET VARIABLE DISTRIBUTION:\n",
      "   • UNKNOWN (-1): 39 (0.8%)\n",
      "   • BAIK (0): 618 (11.9%)\n",
      "   • SEDANG (1): 3,863 (74.6%)\n",
      "   • TIDAK SEHAT (2): 651 (12.6%)\n",
      "   • SANGAT TIDAK SEHAT (3): 4 (0.1%)\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 19: Summary Statistics and Data Quality Report\n",
    "# =============================================================================\n",
    "\n",
    "def generate_data_quality_report(df):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive data quality report.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"DATA QUALITY REPORT - MASTER DATAFRAME\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\n📊 BASIC INFO:\")\n",
    "    print(f\"   • Total records: {len(df):,}\")\n",
    "    print(f\"   • Total features: {len(df.columns)}\")\n",
    "    print(f\"   • Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    print(f\"\\n📅 TEMPORAL COVERAGE:\")\n",
    "    if 'tanggal' in df.index.names:\n",
    "        dates = df.index.get_level_values('tanggal')\n",
    "    else:\n",
    "        dates = df['tanggal']\n",
    "    print(f\"   • Date range: {dates.min().date()} to {dates.max().date()}\")\n",
    "    print(f\"   • Total days: {(dates.max() - dates.min()).days + 1}\")\n",
    "    \n",
    "    print(f\"\\n🏢 STATION COVERAGE:\")\n",
    "    if 'stasiun_id' in df.index.names:\n",
    "        stations = df.index.get_level_values('stasiun_id').unique()\n",
    "    else:\n",
    "        stations = df['stasiun_id'].unique()\n",
    "    for station in sorted(stations):\n",
    "        if 'stasiun_id' in df.index.names:\n",
    "            count = len(df.xs(station, level='stasiun_id'))\n",
    "        else:\n",
    "            count = len(df[df['stasiun_id'] == station])\n",
    "        print(f\"   • {station}: {count:,} records\")\n",
    "    \n",
    "    print(f\"\\n⚠️ MISSING VALUES:\")\n",
    "    missing_cols = []\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        missing = df[col].isna().sum()\n",
    "        if missing > 0:\n",
    "            missing_cols.append((col, missing, missing/len(df)*100))\n",
    "    \n",
    "    if missing_cols:\n",
    "        for col, count, pct in sorted(missing_cols, key=lambda x: -x[1])[:10]:\n",
    "            print(f\"   • {col}: {count:,} ({pct:.1f}%)\")\n",
    "    else:\n",
    "        print(\"   • No missing values in numeric columns!\")\n",
    "    \n",
    "    print(f\"\\n🎯 TARGET VARIABLE DISTRIBUTION:\")\n",
    "    if 'kategori_encoded' in df.columns:\n",
    "        target_dist = df['kategori_encoded'].value_counts().sort_index()\n",
    "        category_names = {0: 'BAIK', 1: 'SEDANG', 2: 'TIDAK SEHAT', 3: 'SANGAT TIDAK SEHAT', 4: 'BERBAHAYA', -1: 'UNKNOWN'}\n",
    "        for val, count in target_dist.items():\n",
    "            name = category_names.get(int(val), 'UNKNOWN')\n",
    "            print(f\"   • {name} ({int(val)}): {count:,} ({count/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# Generate report\n",
    "generate_data_quality_report(df_master_scaled_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98890cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved: ../processed_data/master_df_scaled.csv\n",
      "✓ Saved: ../processed_data/master_df_unscaled.csv\n",
      "✓ Saved: ../processed_data/scaler.pkl\n",
      "✓ Saved: ../processed_data/target_encoder.pkl\n",
      "✓ Saved: ../processed_data/preprocessing_metadata.pkl\n",
      "\n",
      "✅ All preprocessing complete! Files saved to: ../processed_data/\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 20: Export Preprocessed Data\n",
    "# =============================================================================\n",
    "\n",
    "# Export both scaled and unscaled versions\n",
    "output_dir = \"../processed_data/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save master dataframe (scaled)\n",
    "df_master_scaled.to_csv(os.path.join(output_dir, \"master_df_scaled.csv\"), index=False)\n",
    "print(f\"✓ Saved: {output_dir}master_df_scaled.csv\")\n",
    "\n",
    "# Save master dataframe (unscaled) - useful for EDA\n",
    "df_master.to_csv(os.path.join(output_dir, \"master_df_unscaled.csv\"), index=False)\n",
    "print(f\"✓ Saved: {output_dir}master_df_unscaled.csv\")\n",
    "\n",
    "# Save scaler for later use in prediction\n",
    "import pickle\n",
    "with open(os.path.join(output_dir, \"scaler.pkl\"), 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"✓ Saved: {output_dir}scaler.pkl\")\n",
    "\n",
    "# Save target encoder\n",
    "with open(os.path.join(output_dir, \"target_encoder.pkl\"), 'wb') as f:\n",
    "    pickle.dump(target_encoder, f)\n",
    "print(f\"✓ Saved: {output_dir}target_encoder.pkl\")\n",
    "\n",
    "# Save column lists for reference\n",
    "preprocessing_metadata = {\n",
    "    'scaled_columns': scaled_columns,\n",
    "    'pollutant_columns': pollutant_columns,\n",
    "    'lag_columns': lag_columns,\n",
    "    'lag_periods': lag_periods,\n",
    "    'rolling_columns': rolling_columns,\n",
    "    'window_size': window_size,\n",
    "    'target_categories': ['BAIK', 'SEDANG', 'TIDAK SEHAT', 'SANGAT TIDAK SEHAT', 'BERBAHAYA'],\n",
    "    'station_coords': STATION_COORDS,\n",
    "    'kota_to_station_map': KOTA_TO_STATION\n",
    "}\n",
    "\n",
    "with open(os.path.join(output_dir, \"preprocessing_metadata.pkl\"), 'wb') as f:\n",
    "    pickle.dump(preprocessing_metadata, f)\n",
    "print(f\"✓ Saved: {output_dir}preprocessing_metadata.pkl\")\n",
    "\n",
    "print(f\"\\n✅ All preprocessing complete! Files saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62918f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 FEATURE SUMMARY BY CATEGORY\n",
      "======================================================================\n",
      "\n",
      "IDENTIFIER (3 features):\n",
      "  ['tanggal', 'stasiun_id', 'stasiun']\n",
      "\n",
      "POLLUTANT (7 features):\n",
      "  ['pm_sepuluh', 'pm_duakomalima', 'sulfur_dioksida', 'karbon_monoksida', 'ozon', 'nitrogen_dioksida', 'max']\n",
      "\n",
      "OTHER (11 features):\n",
      "  ['parameter_pencemar_kritis', 'wind_direction_alt', 'humidity_max', 'humidity_min', 'cloud_cover_max', 'cloud_cover_min', 'wind_gusts_mean', 'wind_gusts_min', 'wind_speed_min', 'pressure_max', 'pressure_min']\n",
      "\n",
      "TARGET (2 features):\n",
      "  ['kategori', 'kategori_encoded']\n",
      "\n",
      "TIME FEATURE (4 features):\n",
      "  ['year', 'month', 'is_weekend', 'is_holiday_nasional']\n",
      "\n",
      "LAG FEATURE (4 features):\n",
      "  ['pm_sepuluh_lag_1d', 'pm_sepuluh_lag_7d', 'pm_duakomalima_lag_1d', 'pm_duakomalima_lag_7d']\n",
      "\n",
      "WEATHER (13 features):\n",
      "  ['temp_max', 'temp_min', 'precipitation_sum', 'precipitation_hours', 'wind_speed_max', 'wind_direction_10m_dominant', 'radiation_sum', 'temp_mean', 'humidity_mean', 'cloud_cover_mean', 'pressure_mean', 'wind_gusts_max', 'wind_speed_mean']\n",
      "\n",
      "CIRCULAR ENCODING (2 features):\n",
      "  ['wind_sin', 'wind_cos']\n",
      "\n",
      "ROLLING FEATURE (2 features):\n",
      "  ['precipitation_sum_rolling_3d_mean', 'temp_mean_rolling_3d_mean']\n",
      "\n",
      "POPULATION (1 features):\n",
      "  ['jumlah_penduduk']\n",
      "\n",
      "VEGETATION INDEX (1 features):\n",
      "  ['ndvi']\n",
      "\n",
      "RIVER QUALITY (5 features):\n",
      "  ['pH', 'BOD', 'COD', 'DO', 'TSS']\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 21: Feature Summary Table\n",
    "# =============================================================================\n",
    "\n",
    "# Create a summary of all features in the master dataframe\n",
    "feature_summary = pd.DataFrame({\n",
    "    'Feature': df_master_scaled.columns,\n",
    "    'Type': [str(df_master_scaled[col].dtype) for col in df_master_scaled.columns],\n",
    "    'Non-Null': [df_master_scaled[col].notna().sum() for col in df_master_scaled.columns],\n",
    "    'Null %': [df_master_scaled[col].isna().sum() / len(df_master_scaled) * 100 for col in df_master_scaled.columns],\n",
    "})\n",
    "\n",
    "# Add category for each feature\n",
    "def categorize_feature(col):\n",
    "    if col in ['tanggal', 'stasiun_id', 'stasiun']:\n",
    "        return 'Identifier'\n",
    "    elif col in ['kategori', 'kategori_encoded']:\n",
    "        return 'Target'\n",
    "    elif col in pollutant_columns:\n",
    "        return 'Pollutant'\n",
    "    elif 'lag' in col:\n",
    "        return 'Lag Feature'\n",
    "    elif 'rolling' in col:\n",
    "        return 'Rolling Feature'\n",
    "    elif col in ['wind_sin', 'wind_cos']:\n",
    "        return 'Circular Encoding'\n",
    "    elif col in ['month', 'is_weekend', 'is_holiday_nasional', 'year']:\n",
    "        return 'Time Feature'\n",
    "    elif col in ['temp_max', 'temp_min', 'temp_mean', 'precipitation_sum', 'precipitation_hours',\n",
    "                 'wind_speed_max', 'wind_speed_mean', 'humidity_mean', 'pressure_mean', \n",
    "                 'cloud_cover_mean', 'radiation_sum', 'wind_gusts_max', 'wind_direction_10m_dominant']:\n",
    "        return 'Weather'\n",
    "    elif col in ['pH', 'BOD', 'COD', 'DO', 'TSS']:\n",
    "        return 'River Quality'\n",
    "    elif col == 'ndvi':\n",
    "        return 'Vegetation Index'\n",
    "    elif col == 'jumlah_penduduk':\n",
    "        return 'Population'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "feature_summary['Category'] = feature_summary['Feature'].apply(categorize_feature)\n",
    "\n",
    "print(\"📋 FEATURE SUMMARY BY CATEGORY\")\n",
    "print(\"=\" * 70)\n",
    "for category in feature_summary['Category'].unique():\n",
    "    features_in_cat = feature_summary[feature_summary['Category'] == category]['Feature'].tolist()\n",
    "    print(f\"\\n{category.upper()} ({len(features_in_cat)} features):\")\n",
    "    print(f\"  {features_in_cat}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
