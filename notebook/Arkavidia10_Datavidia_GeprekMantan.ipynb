{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a38ae8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5fab50bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memulai pemrosesan data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_28544\\3755585498.py:39: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  return pd.to_datetime(x)\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_28544\\3755585498.py:39: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  return pd.to_datetime(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melatih model machine learning...\n",
      "Membuat prediksi...\n",
      "Selesai! File 'submission_final.csv' telah berhasil dibuat.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "\n",
    "# 1. DEFINISI FILE (Sesuai cara panggil yang Anda minta)\n",
    "files_dict = {\n",
    "    2010: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2010-komponen-data.csv\",\n",
    "    2011: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2011-komponen-data.csv\",\n",
    "    2012: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2012-komponen-data.csv\",\n",
    "    2013: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2013-komponen-data.csv\",\n",
    "    2014: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2014-komponen-data.csv\",\n",
    "    2015: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2015-komponen-data.csv\",\n",
    "    2016: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2016-komponen-data.csv\",\n",
    "    2017: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2017-komponen-data.csv\",\n",
    "    2018: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2018-komponen-data.csv\",\n",
    "    2019: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2019-komponen-data.csv\",\n",
    "    2020: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2020-komponen-data.csv\",\n",
    "    2021: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2021-komponen-data.csv\",\n",
    "    2022: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2022-komponen-data.csv\",\n",
    "    2023: \"ISPU/data-indeks-standar-pencemar-udara-(ispu)-di-provinsi-dki-jakarta-2023-komponen-data.csv\",\n",
    "    2024: \"ISPU/data-indeks-standar-pencemar-udara-(ispu)-di-provinsi-dki-jakarta-komponen-data-2024.csv\",\n",
    "    2025: \"ISPU/data-indeks-standar-pencemar-udara-(ispu)-di-provinsi-dki-jakarta-komponen-data-2025.csv\"\n",
    "}\n",
    "\n",
    "# 2. FUNGSI PEMBANTU (CLEANING)\n",
    "def clean_stasiun(s):\n",
    "    s = str(s).upper()\n",
    "    for code in ['DKI1', 'DKI2', 'DKI3', 'DKI4', 'DKI5']:\n",
    "        if code in s: return code\n",
    "    return 'UNKNOWN'\n",
    "\n",
    "def parse_date(x):\n",
    "    try:\n",
    "        # Menangani angka serial Excel (misal di data 2022)\n",
    "        if isinstance(x, (int, float)) or (isinstance(x, str) and x.replace('.','').isdigit()):\n",
    "            return pd.to_datetime('1899-12-30') + pd.to_timedelta(float(x), unit='D')\n",
    "        return pd.to_datetime(x)\n",
    "    except:\n",
    "        return pd.NaT\n",
    "\n",
    "all_data = []\n",
    "\n",
    "# 3. LOOPING PROSES DATA\n",
    "print(\"Memulai pemrosesan data...\")\n",
    "for year, filename in files_dict.items():\n",
    "    if not os.path.exists(filename): continue\n",
    "    \n",
    "    df = pd.read_csv(filename)\n",
    "    df.columns = [c.lower() for c in df.columns]\n",
    "    \n",
    "    # Identifikasi kolom lokasi dan kategori yang berbeda tiap tahun\n",
    "    st_col = 'stasiun' if 'stasiun' in df.columns else 'lokasi_spku'\n",
    "    cat_col = 'kategori' if 'kategori' in df.columns else 'categori'\n",
    "    \n",
    "    # Penanganan khusus tahun 2024-2025 (tanggal dipisah)\n",
    "    if year >= 2024:\n",
    "        df['dt_final'] = pd.to_datetime(df['periode_data'].astype(str) + \n",
    "                                        df['tanggal'].astype(str).str.zfill(2), \n",
    "                                        format='%Y%m%d', errors='coerce')\n",
    "    else:\n",
    "        df['dt_final'] = df['tanggal'].apply(parse_date)\n",
    "    \n",
    "    # Standardisasi hasil\n",
    "    df['st_unified'] = df[st_col].apply(clean_stasiun)\n",
    "    df['cat_unified'] = df[cat_col].astype(str).str.upper().str.strip()\n",
    "    \n",
    "    all_data.append(df[['dt_final', 'st_unified', 'cat_unified']])\n",
    "\n",
    "# 4. PENGGABUNGAN & FEATURE ENGINEERING\n",
    "master = pd.concat(all_data).dropna(subset=['dt_final'])\n",
    "master = master[(master['cat_unified'] != 'TIDAK ADA DATA') & (master['st_unified'] != 'UNKNOWN')]\n",
    "\n",
    "master['month'] = master['dt_final'].dt.month\n",
    "master['day'] = master['dt_final'].dt.day\n",
    "master['day_of_week'] = master['dt_final'].dt.dayofweek\n",
    "\n",
    "# Label Encoding untuk Stasiun dan Kategori\n",
    "le_st = LabelEncoder()\n",
    "master['st_enc'] = le_st.fit_transform(master['st_unified'])\n",
    "le_cat = LabelEncoder()\n",
    "master['cat_enc'] = le_cat.fit_transform(master['cat_unified'])\n",
    "\n",
    "# 5. PELATIHAN MODEL\n",
    "print(\"Melatih model machine learning...\")\n",
    "X = master[['st_enc', 'month', 'day', 'day_of_week']]\n",
    "y = master['cat_enc']\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# 6. PREDIKSI UNTUK SUBMISSION\n",
    "print(\"Membuat prediksi...\")\n",
    "sub = pd.read_csv(\"sample_submission.csv\") # Pastikan file ini ada di direktori utama\n",
    "sub['tanggal_part'] = pd.to_datetime(sub['id'].str.split('_').str[0])\n",
    "sub['st_part'] = sub['id'].str.split('_').str[1].apply(clean_stasiun)\n",
    "\n",
    "sub['month'] = sub['tanggal_part'].dt.month\n",
    "sub['day'] = sub['tanggal_part'].dt.day\n",
    "sub['day_of_week'] = sub['tanggal_part'].dt.dayofweek\n",
    "sub['st_enc'] = le_st.transform(sub['st_part'])\n",
    "\n",
    "X_test = sub[['st_enc', 'month', 'day', 'day_of_week']]\n",
    "preds = model.predict(X_test)\n",
    "sub['category'] = le_cat.inverse_transform(preds)\n",
    "\n",
    "# 7. SIMPAN HASIL AKHIR\n",
    "sub[['id', 'category']].to_csv(\"submission_final_RandomForest.csv\", index=False)\n",
    "print(\"Selesai! File 'submission_final.csv' telah berhasil dibuat.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb1239ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluasi Model Random Forest ---\n",
      "Macro F1-Score: 0.2808\n",
      "\n",
      "Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "              BAIK       0.27      0.26      0.27       441\n",
      "SANGAT TIDAK SEHAT       0.00      0.00      0.00        39\n",
      "            SEDANG       0.68      0.76      0.72      2079\n",
      "       TIDAK SEHAT       0.18      0.11      0.14       524\n",
      "\n",
      "          accuracy                           0.57      3083\n",
      "         macro avg       0.28      0.28      0.28      3083\n",
      "      weighted avg       0.53      0.57      0.55      3083\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# 1. Bagi data master menjadi Training dan Validation set (80:20)\n",
    "# Menggunakan X dan y yang sudah didefinisikan di cell sebelumnya\n",
    "X_train_eval, X_val, y_train_eval, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. Latih model khusus untuk evaluasi\n",
    "clf_eval = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf_eval.fit(X_train_eval, y_train_eval)\n",
    "\n",
    "# 3. Lakukan prediksi pada data validasi\n",
    "y_pred_val = clf_eval.predict(X_val)\n",
    "\n",
    "# 4. Hitung Macro F1-Score\n",
    "macro_f1 = f1_score(y_val, y_pred_val, average='macro')\n",
    "\n",
    "# 5. Tampilkan Hasil\n",
    "print(f\"--- Evaluasi Model Random Forest ---\")\n",
    "print(f\"Macro F1-Score: {macro_f1:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "\n",
    "# Menggunakan le_cat (sesuai dengan kode lengkap di cell sebelumnya)\n",
    "present_classes = np.unique(np.concatenate([y_val, y_pred_val]))\n",
    "present_names = le_cat.inverse_transform(present_classes)\n",
    "\n",
    "print(classification_report(y_val, y_pred_val, labels=present_classes, target_names=present_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18f19eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Skor Model KNN Optimized ---\n",
      "Macro F1-Score: 0.2838\n",
      "\n",
      "Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "              BAIK       0.27      0.26      0.27       441\n",
      "SANGAT TIDAK SEHAT       0.00      0.00      0.00        39\n",
      "            SEDANG       0.68      0.77      0.72      2079\n",
      "       TIDAK SEHAT       0.21      0.11      0.15       524\n",
      "\n",
      "          accuracy                           0.58      3083\n",
      "         macro avg       0.29      0.29      0.28      3083\n",
      "      weighted avg       0.53      0.58      0.55      3083\n",
      "\n",
      "\n",
      "--- Membuat File Submission ---\n",
      "Selesai! File 'submission_final_knn.csv' telah berhasil dibuat.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- BAGIAN 1: EVALUASI (UNTUK MELIHAT SKOR) ---\n",
    "\n",
    "# 1. Menyiapkan Fitur Cyclical (Sin/Cos) pada Data Master\n",
    "master['month_sin'] = np.sin(2 * np.pi * master['month']/12)\n",
    "master['month_cos'] = np.cos(2 * np.pi * master['month']/12)\n",
    "\n",
    "# 2. Update Fitur dan Target\n",
    "features_eval = ['st_enc', 'month_sin', 'month_cos', 'day', 'day_of_week']\n",
    "X_final_eval = master[features_eval]\n",
    "y_final_eval = master['cat_enc']\n",
    "\n",
    "# 3. Bagi data menjadi Train dan Validation set (80:20)\n",
    "X_train_knn, X_val_knn, y_train_knn, y_val_knn = train_test_split(X_final_eval, y_final_eval, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. Scaling\n",
    "scaler_eval = StandardScaler()\n",
    "X_train_scaled = scaler_eval.fit_transform(X_train_knn)\n",
    "X_val_scaled = scaler_eval.transform(X_val_knn)\n",
    "\n",
    "# 5. Latih Model untuk Evaluasi\n",
    "knn_eval = KNeighborsClassifier(n_neighbors=7, weights='distance', metric='manhattan')\n",
    "knn_eval.fit(X_train_scaled, y_train_knn)\n",
    "\n",
    "# 6. Prediksi & Tampilkan Skor\n",
    "y_pred_knn = knn_eval.predict(X_val_scaled)\n",
    "macro_f1_knn = f1_score(y_val_knn, y_pred_knn, average='macro')\n",
    "\n",
    "print(f\"--- Skor Model KNN Optimized ---\")\n",
    "print(f\"Macro F1-Score: {macro_f1_knn:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "present_classes = np.unique(np.concatenate([y_val_knn, y_pred_knn]))\n",
    "present_names = le_cat.inverse_transform(present_classes)\n",
    "print(classification_report(y_val_knn, y_pred_knn, labels=present_classes, target_names=present_names))\n",
    "\n",
    "\n",
    "# --- BAGIAN 2: MEMBUAT FILE SUBMISSION (MENGGUNAKAN SELURUH DATA) ---\n",
    "\n",
    "print(\"\\n--- Membuat File Submission ---\")\n",
    "\n",
    "# 1. Latih ulang model menggunakan SELURUH data agar lebih akurat\n",
    "scaler_final = StandardScaler()\n",
    "X_master_scaled = scaler_final.fit_transform(X_final_eval)\n",
    "knn_final = KNeighborsClassifier(n_neighbors=7, weights='distance', metric='manhattan')\n",
    "knn_final.fit(X_master_scaled, y_final_eval)\n",
    "\n",
    "# 2. Baca Sample Submission\n",
    "sub = pd.read_csv(\"sample_submission.csv\")\n",
    "\n",
    "# 3. Preprocessing Data Test (Ekstraksi Fitur dari ID)\n",
    "sub['tanggal_dt'] = pd.to_datetime(sub['id'].str.split('_').str[0])\n",
    "sub['stasiun_nm'] = sub['id'].str.split('_').str[1]\n",
    "\n",
    "sub['month'] = sub['tanggal_dt'].dt.month\n",
    "sub['day'] = sub['tanggal_dt'].dt.day\n",
    "sub['day_of_week'] = sub['tanggal_dt'].dt.dayofweek\n",
    "sub['month_sin'] = np.sin(2 * np.pi * sub['month']/12)\n",
    "sub['month_cos'] = np.cos(2 * np.pi * sub['month']/12)\n",
    "\n",
    "# Gunakan fungsi clean_stasiun yang sudah kita buat sebelumnya untuk encoding\n",
    "sub['st_enc'] = le_st.transform(sub['stasiun_nm'].apply(clean_stasiun))\n",
    "\n",
    "# 4. Scaling Data Test\n",
    "X_test_final = sub[features_eval]\n",
    "X_test_scaled = scaler_final.transform(X_test_final)\n",
    "\n",
    "# 5. Prediksi Final\n",
    "preds_final = knn_final.predict(X_test_scaled)\n",
    "sub['category'] = le_cat.inverse_transform(preds_final)\n",
    "\n",
    "# 6. Simpan ke CSV\n",
    "sub[['id', 'category']].to_csv(\"submission_final_knn.csv\", index=False)\n",
    "print(\"Selesai! File 'submission_final_knn.csv' telah berhasil dibuat.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "507c8239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memuat data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_28544\\3962650774.py:45: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  return pd.to_datetime(val)\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_28544\\3962650774.py:45: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  return pd.to_datetime(val)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- EVALUASI PERFORMA MODEL ---\n",
      "Macro F1-Score: 0.2943\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "              BAIK       0.32      0.28      0.30       492\n",
      "SANGAT TIDAK SEHAT       0.00      0.00      0.00        42\n",
      "            SEDANG       0.70      0.77      0.73      2176\n",
      "       TIDAK SEHAT       0.18      0.13      0.15       463\n",
      "\n",
      "          accuracy                           0.59      3173\n",
      "         macro avg       0.30      0.29      0.29      3173\n",
      "      weighted avg       0.55      0.59      0.57      3173\n",
      "\n",
      "\n",
      "Membuat file submission...\n",
      "Selesai! File 'submission_final_2025.csv' telah siap.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import os\n",
    "\n",
    "# 1. MEMANGGIL DATA (Sesuai instruksi Anda, dengan file 2025 terbaru)\n",
    "print(\"Memuat data...\")\n",
    "ISPU2010 = pd.read_csv(\"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2010-komponen-data.csv\")\n",
    "ISPU2011 = pd.read_csv(\"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2011-komponen-data.csv\")\n",
    "ISPU2012 = pd.read_csv(\"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2012-komponen-data.csv\")\n",
    "ISPU2013 = pd.read_csv(\"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2013-komponen-data.csv\")\n",
    "ISPU2014 = pd.read_csv(\"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2014-komponen-data.csv\")\n",
    "ISPU2015 = pd.read_csv(\"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2015-komponen-data.csv\")\n",
    "ISPU2016 = pd.read_csv(\"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2016-komponen-data.csv\")\n",
    "ISPU2017 = pd.read_csv(\"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2017-komponen-data.csv\")\n",
    "ISPU2018 = pd.read_csv(\"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2018-komponen-data.csv\")\n",
    "ISPU2019 = pd.read_csv(\"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2019-komponen-data.csv\")\n",
    "ISPU2020 = pd.read_csv(\"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2020-komponen-data.csv\")\n",
    "ISPU2021 = pd.read_csv(\"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2021-komponen-data.csv\")\n",
    "ISPU2022 = pd.read_csv(\"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2022-komponen-data.csv\")\n",
    "ISPU2023 = pd.read_csv(\"ISPU/data-indeks-standar-pencemar-udara-(ispu)-di-provinsi-dki-jakarta-2023-komponen-data.csv\")\n",
    "ISPU2024 = pd.read_csv(\"ISPU/data-indeks-standar-pencemar-udara-(ispu)-di-provinsi-dki-jakarta-komponen-data-2024.csv\")\n",
    "# Menggunakan file terbaru yang Anda berikan\n",
    "ISPU2025 = pd.read_csv(\"data_ispu_2025.csv\") \n",
    "\n",
    "# 2. FUNGSI PREPROCESSING UNTUK MENYAMAKAN SKEMA\n",
    "def clean_stasiun(s):\n",
    "    s = str(s).upper()\n",
    "    for code in ['DKI1', 'DKI2', 'DKI3', 'DKI4', 'DKI5']:\n",
    "        if code in s: return code\n",
    "    return 'UNKNOWN'\n",
    "\n",
    "def parse_date(row, year):\n",
    "    try:\n",
    "        # Penanganan khusus 2024-2025\n",
    "        if year >= 2024:\n",
    "            return pd.to_datetime(str(row['periode_data']) + str(int(row['tanggal'])).zfill(2), format='%Y%m%d')\n",
    "        # Penanganan angka serial excel (2022)\n",
    "        val = row['tanggal']\n",
    "        if isinstance(val, (int, float)) or (isinstance(val, str) and val.replace('.','').isdigit()):\n",
    "            return pd.to_datetime('1899-12-30') + pd.to_timedelta(float(val), unit='D')\n",
    "        return pd.to_datetime(val)\n",
    "    except: return pd.NaT\n",
    "\n",
    "# Daftar dataframe dan tahunnya\n",
    "df_list = [\n",
    "    (ISPU2010, 2010), (ISPU2011, 2011), (ISPU2012, 2012), (ISPU2013, 2013),\n",
    "    (ISPU2014, 2014), (ISPU2015, 2015), (ISPU2016, 2016), (ISPU2017, 2017),\n",
    "    (ISPU2018, 2018), (ISPU2019, 2019), (ISPU2020, 2020), (ISPU2021, 2021),\n",
    "    (ISPU2022, 2022), (ISPU2023, 2023), (ISPU2024, 2024), (ISPU2025, 2025)\n",
    "]\n",
    "\n",
    "processed_dfs = []\n",
    "for df, year in df_list:\n",
    "    df.columns = [c.lower() for c in df.columns]\n",
    "    st_col = 'stasiun' if 'stasiun' in df.columns else 'lokasi_spku'\n",
    "    cat_col = 'kategori' if 'kategori' in df.columns else 'categori'\n",
    "    \n",
    "    df['dt_final'] = df.apply(lambda r: parse_date(r, year), axis=1)\n",
    "    df['st_unified'] = df[st_col].apply(clean_stasiun)\n",
    "    df['cat_unified'] = df[cat_col].astype(str).str.upper().str.strip()\n",
    "    \n",
    "    processed_dfs.append(df[['dt_final', 'st_unified', 'cat_unified']])\n",
    "\n",
    "# 3. GABUNG DAN FEATURE ENGINEERING\n",
    "master = pd.concat(processed_dfs).dropna(subset=['dt_final'])\n",
    "master = master[(master['cat_unified'] != 'TIDAK ADA DATA') & (master['st_unified'] != 'UNKNOWN')]\n",
    "\n",
    "# Ekstraksi Fitur Waktu & Cyclical Encoding\n",
    "master['month'] = master['dt_final'].dt.month\n",
    "master['day'] = master['dt_final'].dt.day\n",
    "master['day_of_week'] = master['dt_final'].dt.dayofweek\n",
    "master['month_sin'] = np.sin(2 * np.pi * master['month']/12)\n",
    "master['month_cos'] = np.cos(2 * np.pi * master['month']/12)\n",
    "\n",
    "# Encoding Label\n",
    "le_st = LabelEncoder()\n",
    "master['st_enc'] = le_st.fit_transform(master['st_unified'])\n",
    "le_cat = LabelEncoder()\n",
    "master['cat_enc'] = le_cat.fit_transform(master['cat_unified'])\n",
    "\n",
    "# 4. EVALUASI MODEL (MACRO F1-SCORE)\n",
    "features = ['st_enc', 'month_sin', 'month_cos', 'day', 'day_of_week']\n",
    "X = master[features]\n",
    "y = master['cat_enc']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Menggunakan KNN sesuai referensi jurnal\n",
    "knn = KNeighborsClassifier(n_neighbors=7, weights='distance', metric='manhattan')\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = knn.predict(X_val_scaled)\n",
    "print(f\"\\n--- EVALUASI PERFORMA MODEL ---\")\n",
    "print(f\"Macro F1-Score: {f1_score(y_val, y_pred, average='macro'):.4f}\")\n",
    "\n",
    "# Menampilkan laporan klasifikasi yang aman dari error label\n",
    "unique_labels = np.unique(np.concatenate([y_val, y_pred]))\n",
    "print(classification_report(y_val, y_pred, labels=unique_labels, target_names=le_cat.inverse_transform(unique_labels)))\n",
    "\n",
    "# 5. MEMBUAT FILE SUBMISSION FINAL\n",
    "print(\"\\nMembuat file submission...\")\n",
    "# Latih ulang pada 100% data untuk hasil terbaik\n",
    "scaler_final = StandardScaler()\n",
    "X_master_scaled = scaler_final.fit_transform(X)\n",
    "knn.fit(X_master_scaled, y)\n",
    "\n",
    "sub = pd.read_csv(\"sample_submission.csv\")\n",
    "sub['tanggal_dt'] = pd.to_datetime(sub['id'].str.split('_').str[0])\n",
    "sub['stasiun_nm'] = sub['id'].str.split('_').str[1]\n",
    "\n",
    "sub['month'] = sub['tanggal_dt'].dt.month\n",
    "sub['day'] = sub['tanggal_dt'].dt.day\n",
    "sub['day_of_week'] = sub['tanggal_dt'].dt.dayofweek\n",
    "sub['month_sin'] = np.sin(2 * np.pi * sub['month']/12)\n",
    "sub['month_cos'] = np.cos(2 * np.pi * sub['month']/12)\n",
    "sub['st_enc'] = le_st.transform(sub['stasiun_nm'].apply(clean_stasiun))\n",
    "\n",
    "X_test_scaled = scaler_final.transform(sub[features])\n",
    "sub['category'] = le_cat.inverse_transform(knn.predict(X_test_scaled))\n",
    "\n",
    "sub[['id', 'category']].to_csv(\"submission_final_knn2.csv\", index=False)\n",
    "print(\"Selesai! File 'submission_final_2025.csv' telah siap.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c30bb891",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_28544\\4195884875.py:45: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  return pd.to_datetime(val)\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_28544\\4195884875.py:45: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  return pd.to_datetime(val)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1-Score: 0.9704\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "              BAIK       1.00      1.00      1.00       481\n",
      "SANGAT TIDAK SEHAT       0.92      0.90      0.91        40\n",
      "            SEDANG       0.99      1.00      1.00      2163\n",
      "       TIDAK SEHAT       0.98      0.97      0.98       489\n",
      "\n",
      "          accuracy                           0.99      3173\n",
      "         macro avg       0.97      0.97      0.97      3173\n",
      "      weighted avg       0.99      0.99      0.99      3173\n",
      "\n",
      "Selesai! File 'submission_xgb_final.csv' telah dibuat.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import os\n",
    "\n",
    "# 1. DAFTAR FILE DAN PEMUATAN DATA\n",
    "files_dict = {\n",
    "    2010: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2010-komponen-data.csv\",\n",
    "    2011: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2011-komponen-data.csv\",\n",
    "    2012: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2012-komponen-data.csv\",\n",
    "    2013: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2013-komponen-data.csv\",\n",
    "    2014: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2014-komponen-data.csv\",\n",
    "    2015: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2015-komponen-data.csv\",\n",
    "    2016: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2016-komponen-data.csv\",\n",
    "    2017: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2017-komponen-data.csv\",\n",
    "    2018: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2018-komponen-data.csv\",\n",
    "    2019: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2019-komponen-data.csv\",\n",
    "    2020: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2020-komponen-data.csv\",\n",
    "    2021: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2021-komponen-data.csv\",\n",
    "    2022: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2022-komponen-data.csv\",\n",
    "    2023: \"ISPU/data-indeks-standar-pencemar-udara-(ispu)-di-provinsi-dki-jakarta-2023-komponen-data.csv\",\n",
    "    2024: \"ISPU/data-indeks-standar-pencemar-udara-(ispu)-di-provinsi-dki-jakarta-komponen-data-2024.csv\",\n",
    "    2025: \"data_ispu_2025.csv\"\n",
    "}\n",
    "\n",
    "pollutant_cols = ['pm_sepuluh', 'pm_duakomalima', 'sulfur_dioksida', 'karbon_monoksida', 'ozon', 'nitrogen_dioksida']\n",
    "rename_map = {'pm10': 'pm_sepuluh', 'pm_10': 'pm_sepuluh', 'pm25': 'pm_duakomalima', 'pm_25': 'pm_duakomalima',\n",
    "              'so2': 'sulfur_dioksida', 'co': 'karbon_monoksida', 'o3': 'ozon', 'no2': 'nitrogen_dioksida', 'lokasi_spku': 'stasiun'}\n",
    "\n",
    "def clean_stasiun(s):\n",
    "    s = str(s).upper()\n",
    "    for code in ['DKI1', 'DKI2', 'DKI3', 'DKI4', 'DKI5']:\n",
    "        if code in s: return code\n",
    "    return 'UNKNOWN'\n",
    "\n",
    "def parse_date(row, year):\n",
    "    try:\n",
    "        if year >= 2024: return pd.to_datetime(str(row['periode_data']) + str(int(row['tanggal'])).zfill(2), format='%Y%m%d')\n",
    "        val = row['tanggal']\n",
    "        if isinstance(val, (int, float)) or (isinstance(val, str) and val.replace('.','').isdigit()):\n",
    "            return pd.to_datetime('1899-12-30') + pd.to_timedelta(float(val), unit='D')\n",
    "        return pd.to_datetime(val)\n",
    "    except: return pd.NaT\n",
    "\n",
    "# 2. GABUNG MASTER DATA\n",
    "all_data = []\n",
    "for year, filename in files_dict.items():\n",
    "    if not os.path.exists(filename): continue\n",
    "    df = pd.read_csv(filename).rename(columns=str.lower).rename(columns=rename_map)\n",
    "    df['dt_final'] = df.apply(lambda r: parse_date(r, year), axis=1)\n",
    "    df['st_unified'] = df['stasiun'].apply(clean_stasiun)\n",
    "    cat_col = 'kategori' if 'kategori' in df.columns else 'categori'\n",
    "    df['cat_unified'] = df[cat_col].astype(str).str.upper().str.strip()\n",
    "    \n",
    "    for c in pollutant_cols:\n",
    "        if c not in df.columns: df[c] = np.nan\n",
    "        else: df[c] = pd.to_numeric(df[c].astype(str).str.replace(',', '.').replace(['---', '-', '', 'N/A', 'NAN'], np.nan), errors='coerce')\n",
    "    all_data.append(df[['dt_final', 'st_unified', 'cat_unified'] + pollutant_cols])\n",
    "\n",
    "master = pd.concat(all_data, ignore_index=True).dropna(subset=['dt_final'])\n",
    "master = master[~master['cat_unified'].isin(['TIDAK ADA DATA', 'NAN', '', 'NONE'])]\n",
    "master = master[master['st_unified'] != 'UNKNOWN']\n",
    "\n",
    "# --- SOLUSI VALUE ERROR (Urutan Label) & STRATIFY ERROR ---\n",
    "# Hapus kategori yang jumlahnya terlalu sedikit (< 2) agar bisa di-split (stratify)\n",
    "master = master[master.groupby('cat_unified')['cat_unified'].transform('count') > 1]\n",
    "\n",
    "# Re-Encoding agar label urut 0, 1, 2, 3... (Wajib untuk XGBoost)\n",
    "le_st = LabelEncoder()\n",
    "master['st_enc'] = le_st.fit_transform(master['st_unified'])\n",
    "le_cat = LabelEncoder()\n",
    "master['cat_enc'] = le_cat.fit_transform(master['cat_unified'])\n",
    "\n",
    "# 3. TRAINING XGBOOST\n",
    "features = ['st_enc'] + pollutant_cols\n",
    "X = master[features]\n",
    "y = master['cat_enc']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "xgb_model = XGBClassifier(n_estimators=300, learning_rate=0.05, max_depth=6, random_state=42, eval_metric='mlogloss')\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# EVALUASI\n",
    "y_pred = xgb_model.predict(X_val)\n",
    "print(f\"Macro F1-Score: {f1_score(y_val, y_pred, average='macro'):.4f}\")\n",
    "print(classification_report(y_val, y_pred, target_names=le_cat.classes_))\n",
    "\n",
    "# 4. SUBMISSION\n",
    "sub = pd.read_csv(\"sample_submission.csv\")\n",
    "sub['dt'] = pd.to_datetime(sub['id'].str.split('_').str[0])\n",
    "sub['st_unified'] = sub['id'].str.split('_').str[1].apply(clean_stasiun)\n",
    "\n",
    "# Merge dengan data 2025 untuk ambil polutan\n",
    "df_2025_processed = all_data[-1]\n",
    "sub_joined = sub.merge(df_2025_processed, left_on=['dt', 'st_unified'], right_on=['dt_final', 'st_unified'], how='left')\n",
    "sub_joined['st_enc'] = le_st.transform(sub_joined['st_unified'])\n",
    "\n",
    "sub['category'] = le_cat.inverse_transform(xgb_model.predict(sub_joined[features]))\n",
    "sub[['id', 'category']].to_csv(\"submission_xgb_final.csv\", index=False)\n",
    "print(\"Selesai! File 'submission_xgb_final.csv' telah dibuat.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "84f375ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memproses file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_28544\\1762871055.py:46: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  return pd.to_datetime(val)\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_28544\\1762871055.py:46: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  return pd.to_datetime(val)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Membuat fitur lag...\n",
      "\n",
      "Macro F1-Score: 0.5838\n",
      "\n",
      "Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "              BAIK       0.44      0.59      0.50       122\n",
      "SANGAT TIDAK SEHAT       0.00      0.00      0.00         0\n",
      "            SEDANG       0.87      0.74      0.80       978\n",
      "       TIDAK SEHAT       0.36      0.59      0.45       159\n",
      "\n",
      "          accuracy                           0.71      1259\n",
      "         macro avg       0.42      0.48      0.44      1259\n",
      "      weighted avg       0.76      0.71      0.73      1259\n",
      "\n",
      "\n",
      "Sukses! File 'submission_final_fixed.csv' telah dibuat.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Andri\\Arkavidia10_Datavidia_GeprekMantan\\myvenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "d:\\Andri\\Arkavidia10_Datavidia_GeprekMantan\\myvenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "d:\\Andri\\Arkavidia10_Datavidia_GeprekMantan\\myvenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "d:\\Andri\\Arkavidia10_Datavidia_GeprekMantan\\myvenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "d:\\Andri\\Arkavidia10_Datavidia_GeprekMantan\\myvenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "d:\\Andri\\Arkavidia10_Datavidia_GeprekMantan\\myvenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "d:\\Andri\\Arkavidia10_Datavidia_GeprekMantan\\myvenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "d:\\Andri\\Arkavidia10_Datavidia_GeprekMantan\\myvenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "d:\\Andri\\Arkavidia10_Datavidia_GeprekMantan\\myvenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import os\n",
    "\n",
    "# 1. KONFIGURASI FILE\n",
    "files_dict = {\n",
    "    2010: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2010-komponen-data.csv\",\n",
    "    2011: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2011-komponen-data.csv\",\n",
    "    2012: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2012-komponen-data.csv\",\n",
    "    2013: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2013-komponen-data.csv\",\n",
    "    2014: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2014-komponen-data.csv\",\n",
    "    2015: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2015-komponen-data.csv\",\n",
    "    2016: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2016-komponen-data.csv\",\n",
    "    2017: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2017-komponen-data.csv\",\n",
    "    2018: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2018-komponen-data.csv\",\n",
    "    2019: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2019-komponen-data.csv\",\n",
    "    2020: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2020-komponen-data.csv\",\n",
    "    2021: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2021-komponen-data.csv\",\n",
    "    2022: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2022-komponen-data.csv\",\n",
    "    2023: \"ISPU/data-indeks-standar-pencemar-udara-(ispu)-di-provinsi-dki-jakarta-2023-komponen-data.csv\",\n",
    "    2024: \"ISPU/data-indeks-standar-pencemar-udara-(ispu)-di-provinsi-dki-jakarta-komponen-data-2024.csv\",\n",
    "    2025: \"data_ispu_2025.csv\"\n",
    "}\n",
    "\n",
    "pollutant_cols = ['pm_sepuluh', 'pm_duakomalima', 'sulfur_dioksida', 'karbon_monoksida', 'ozon', 'nitrogen_dioksida']\n",
    "rename_map = {'pm10': 'pm_sepuluh', 'pm_10': 'pm_sepuluh', 'pm25': 'pm_duakomalima', 'pm_25': 'pm_duakomalima',\n",
    "              'so2': 'sulfur_dioksida', 'co': 'karbon_monoksida', 'o3': 'ozon', 'no2': 'nitrogen_dioksida', 'lokasi_spku': 'stasiun'}\n",
    "\n",
    "def clean_stasiun(s):\n",
    "    s = str(s).upper()\n",
    "    for code in ['DKI1', 'DKI2', 'DKI3', 'DKI4', 'DKI5']:\n",
    "        if code in s: return code\n",
    "    return 'UNKNOWN'\n",
    "\n",
    "def parse_date(row, year):\n",
    "    try:\n",
    "        if year >= 2024:\n",
    "            return pd.to_datetime(str(int(row['periode_data'])) + str(int(row['tanggal'])).zfill(2), format='%Y%m%d')\n",
    "        val = row['tanggal']\n",
    "        if isinstance(val, (int, float)) or (isinstance(val, str) and str(val).replace('.','').isdigit()):\n",
    "            return pd.to_datetime('1899-12-30') + pd.to_timedelta(float(val), unit='D')\n",
    "        return pd.to_datetime(val)\n",
    "    except: return pd.NaT\n",
    "\n",
    "# 2. PEMUATAN DATA (PASTIKAN dt_final TERBENTUK)\n",
    "all_data_list = []\n",
    "print(\"Memproses file...\")\n",
    "for year, filename in files_dict.items():\n",
    "    if not os.path.exists(filename): continue\n",
    "    df = pd.read_csv(filename).rename(columns=str.lower).rename(columns=rename_map)\n",
    "    df['dt_final'] = df.apply(lambda r: parse_date(r, year), axis=1)\n",
    "    df['st_unified'] = df['stasiun'].apply(clean_stasiun)\n",
    "    cat_col = 'kategori' if 'kategori' in df.columns else 'categori'\n",
    "    df['cat_unified'] = df[cat_col].astype(str).str.upper().str.strip()\n",
    "    \n",
    "    for c in pollutant_cols:\n",
    "        if c not in df.columns: df[c] = np.nan\n",
    "        else: df[c] = pd.to_numeric(df[c].astype(str).str.replace(',', '.').replace(['---', '-', '', 'N/A', 'NAN'], np.nan), errors='coerce')\n",
    "    all_data_list.append(df[['dt_final', 'st_unified', 'cat_unified'] + pollutant_cols])\n",
    "\n",
    "master = pd.concat(all_data_list, ignore_index=True).dropna(subset=['dt_final'])\n",
    "master = master[~master['cat_unified'].isin(['TIDAK ADA DATA', 'NAN', '', 'NONE'])]\n",
    "master = master[master['st_unified'] != 'UNKNOWN']\n",
    "\n",
    "# 3. FITUR LAG (DATA KEMARIN)\n",
    "print(\"Membuat fitur lag...\")\n",
    "master = master.sort_values(by=['st_unified', 'dt_final'])\n",
    "for col in pollutant_cols:\n",
    "    master[f'{col}_lag1'] = master.groupby('st_unified')[col].shift(1)\n",
    "\n",
    "master_lag = master.dropna(subset=[f'{c}_lag1' for c in pollutant_cols]).copy()\n",
    "\n",
    "# 4. FILTER KELAS & ENCODING (SOLUSI VALUEERROR)\n",
    "# Pastikan setiap kategori minimal punya 2 data agar bisa dibagi ke Train/Val\n",
    "cat_counts = master_lag['cat_unified'].value_counts()\n",
    "valid_cats = cat_counts[cat_counts >= 2].index\n",
    "master_lag = master_lag[master_lag['cat_unified'].isin(valid_cats)]\n",
    "\n",
    "le_st = LabelEncoder()\n",
    "master_lag['st_enc'] = le_st.fit_transform(master_lag['st_unified'])\n",
    "le_cat = LabelEncoder()\n",
    "master_lag['cat_enc'] = le_cat.fit_transform(master_lag['cat_unified'])\n",
    "\n",
    "# 5. TRAINING\n",
    "lag_features = ['st_enc'] + [f'{c}_lag1' for c in pollutant_cols]\n",
    "X = master_lag[lag_features]\n",
    "y = master_lag['cat_enc']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "clf = HistGradientBoostingClassifier(random_state=42, class_weight='balanced')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 6. EVALUASI (SOLUSI MISMATCH TARGET_NAMES)\n",
    "y_pred = clf.predict(X_val)\n",
    "print(f\"\\nMacro F1-Score: {f1_score(y_val, y_pred, average='macro'):.4f}\")\n",
    "\n",
    "# Teknik Agar Classification Report Tidak Error:\n",
    "# Gunakan parameter 'labels' untuk mendefinisikan semua kelas yang seharusnya ada\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(\n",
    "    y_val, \n",
    "    y_pred, \n",
    "    labels=np.arange(len(le_cat.classes_)), \n",
    "    target_names=le_cat.classes_\n",
    "))\n",
    "\n",
    "# 7. SUBMISSION\n",
    "sub = pd.read_csv(\"sample_submission.csv\")\n",
    "sub['dt'] = pd.to_datetime(sub['id'].str.split('_').str[0])\n",
    "sub['st_unified'] = sub['id'].str.split('_').str[1].apply(clean_stasiun)\n",
    "sub['dt_yesterday'] = sub['dt'] - pd.Timedelta(days=1)\n",
    "\n",
    "sub_joined = sub.merge(master[['dt_final', 'st_unified'] + pollutant_cols], \n",
    "                       left_on=['dt_yesterday', 'st_unified'], \n",
    "                       right_on=['dt_final', 'st_unified'], how='left')\n",
    "\n",
    "for c in pollutant_cols:\n",
    "    sub_joined[f'{c}_lag1'] = sub_joined[c]\n",
    "\n",
    "sub_joined['st_enc'] = le_st.transform(sub_joined['st_unified'])\n",
    "sub['category'] = le_cat.inverse_transform(clf.predict(sub_joined[lag_features]))\n",
    "\n",
    "sub[['id', 'category']].to_csv(\"submission_xgb_final2.csv\", index=False)\n",
    "print(\"\\nSukses! File 'submission_final_fixed.csv' telah dibuat.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c5d7e7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5 Memproses file ISPU 2010-2025...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_28544\\416998025.py:48: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  return pd.to_datetime(val)\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_28544\\416998025.py:48: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  return pd.to_datetime(val)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/5 Membuat fitur Lag & Rolling...\n",
      "3/5 Melatih model HistGradientBoosting...\n",
      "\n",
      "Macro F1-Score: 0.4500\n",
      "\n",
      "Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "              BAIK       0.54      0.48      0.51       122\n",
      "SANGAT TIDAK SEHAT       0.00      0.00      0.00         0\n",
      "            SEDANG       0.84      0.88      0.86       978\n",
      "       TIDAK SEHAT       0.50      0.38      0.43       159\n",
      "\n",
      "          accuracy                           0.78      1259\n",
      "         macro avg       0.47      0.44      0.45      1259\n",
      "      weighted avg       0.77      0.78      0.77      1259\n",
      "\n",
      "\n",
      "4/5 Sinkronisasi fitur untuk submission...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Andri\\Arkavidia10_Datavidia_GeprekMantan\\myvenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "d:\\Andri\\Arkavidia10_Datavidia_GeprekMantan\\myvenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "d:\\Andri\\Arkavidia10_Datavidia_GeprekMantan\\myvenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 Selesai! File 'submission_advanced_vfinal.csv' telah siap.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import os\n",
    "\n",
    "# ==========================================\n",
    "# 1. KONFIGURASI FILE & FUNGSI HELPER\n",
    "# ==========================================\n",
    "files_dict = {\n",
    "    2010: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2010-komponen-data.csv\",\n",
    "    2011: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2011-komponen-data.csv\",\n",
    "    2012: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2012-komponen-data.csv\",\n",
    "    2013: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2013-komponen-data.csv\",\n",
    "    2014: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2014-komponen-data.csv\",\n",
    "    2015: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2015-komponen-data.csv\",\n",
    "    2016: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2016-komponen-data.csv\",\n",
    "    2017: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2017-komponen-data.csv\",\n",
    "    2018: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2018-komponen-data.csv\",\n",
    "    2019: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2019-komponen-data.csv\",\n",
    "    2020: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2020-komponen-data.csv\",\n",
    "    2021: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2021-komponen-data.csv\",\n",
    "    2022: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2022-komponen-data.csv\",\n",
    "    2023: \"ISPU/data-indeks-standar-pencemar-udara-(ispu)-di-provinsi-dki-jakarta-2023-komponen-data.csv\",\n",
    "    2024: \"ISPU/data-indeks-standar-pencemar-udara-(ispu)-di-provinsi-dki-jakarta-komponen-data-2024.csv\",\n",
    "    2025: \"data_ispu_2025.csv\"\n",
    "}\n",
    "\n",
    "pollutant_cols = ['pm_sepuluh', 'pm_duakomalima', 'sulfur_dioksida', 'karbon_monoksida', 'ozon', 'nitrogen_dioksida']\n",
    "rename_map = {'pm10': 'pm_sepuluh', 'pm_10': 'pm_sepuluh', 'pm25': 'pm_duakomalima', 'pm_25': 'pm_duakomalima',\n",
    "              'so2': 'sulfur_dioksida', 'co': 'karbon_monoksida', 'o3': 'ozon', 'no2': 'nitrogen_dioksida', 'lokasi_spku': 'stasiun'}\n",
    "\n",
    "def clean_stasiun(s):\n",
    "    s = str(s).upper()\n",
    "    for code in ['DKI1', 'DKI2', 'DKI3', 'DKI4', 'DKI5']:\n",
    "        if code in s: return code\n",
    "    return 'UNKNOWN'\n",
    "\n",
    "def parse_date(row, year):\n",
    "    try:\n",
    "        if year >= 2024:\n",
    "            return pd.to_datetime(str(int(row['periode_data'])) + str(int(row['tanggal'])).zfill(2), format='%Y%m%d')\n",
    "        val = row['tanggal']\n",
    "        if isinstance(val, (int, float)) or (isinstance(val, str) and str(val).replace('.','').isdigit()):\n",
    "            return pd.to_datetime('1899-12-30') + pd.to_timedelta(float(val), unit='D')\n",
    "        return pd.to_datetime(val)\n",
    "    except: return pd.NaT\n",
    "\n",
    "# ==========================================\n",
    "# 2. PEMUATAN DATA & PEMBERSIHAN\n",
    "# ==========================================\n",
    "all_data_list = []\n",
    "print(\"1/5 Memproses file ISPU 2010-2025...\")\n",
    "\n",
    "for year, filename in files_dict.items():\n",
    "    if not os.path.exists(filename): continue\n",
    "    df = pd.read_csv(filename).rename(columns=str.lower).rename(columns=rename_map)\n",
    "    df['dt_final'] = df.apply(lambda r: parse_date(r, year), axis=1)\n",
    "    df['st_unified'] = df['stasiun'].apply(clean_stasiun)\n",
    "    \n",
    "    # Cari kolom kategori yang benar\n",
    "    cat_col = next((c for c in ['kategori', 'categori'] if c in df.columns), None)\n",
    "    df['cat_unified'] = df[cat_col].astype(str).str.upper().str.strip() if cat_col else 'UNKNOWN'\n",
    "    \n",
    "    for c in pollutant_cols:\n",
    "        if c not in df.columns: df[c] = np.nan\n",
    "        else: df[c] = pd.to_numeric(df[c].astype(str).str.replace(',', '.').replace(['---', '-', '', 'N/A', 'NAN'], np.nan), errors='coerce')\n",
    "    \n",
    "    all_data_list.append(df[['dt_final', 'st_unified', 'cat_unified'] + pollutant_cols])\n",
    "\n",
    "master = pd.concat(all_data_list, ignore_index=True).dropna(subset=['dt_final'])\n",
    "master = master[~master['cat_unified'].isin(['TIDAK ADA DATA', 'NAN', '', 'NONE', 'UNKNOWN'])]\n",
    "master = master[master['st_unified'] != 'UNKNOWN']\n",
    "\n",
    "# ==========================================\n",
    "# 3. ADVANCED FEATURE ENGINEERING (LAG & ROLLING)\n",
    "# ==========================================\n",
    "print(\"2/5 Membuat fitur Lag & Rolling...\")\n",
    "master = master.sort_values(by=['st_unified', 'dt_final'])\n",
    "\n",
    "for col in pollutant_cols:\n",
    "    # Lag 1, 2, 3 (Data 1-3 hari sebelumnya)\n",
    "    for i in [1, 2, 3]:\n",
    "        master[f'{col}_lag{i}'] = master.groupby('st_unified')[col].shift(i)\n",
    "    # Rolling Mean (Tren 3 dan 7 hari terakhir)\n",
    "    master[f'{col}_roll3'] = master.groupby('st_unified')[col].transform(lambda x: x.shift(1).rolling(3).mean())\n",
    "    master[f'{col}_roll7'] = master.groupby('st_unified')[col].transform(lambda x: x.shift(1).rolling(7).mean())\n",
    "\n",
    "# Fitur Waktu\n",
    "master['month'] = master['dt_final'].dt.month\n",
    "master['day_of_week'] = master['dt_final'].dt.dayofweek\n",
    "master['is_weekend'] = master['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Hapus baris yang fitur lag-nya kosong\n",
    "master_feat = master.dropna(subset=[f'{c}_lag1' for c in pollutant_cols]).copy()\n",
    "\n",
    "# Encoding Stasiun (st_enc) - Dilakukan DI SINI agar tidak ada KeyError\n",
    "le_st = LabelEncoder()\n",
    "master_feat['st_enc'] = le_st.fit_transform(master_feat['st_unified'])\n",
    "\n",
    "# Filter kelas langka agar stratify tidak error\n",
    "cat_counts = master_feat['cat_unified'].value_counts()\n",
    "valid_cats = cat_counts[cat_counts >= 2].index\n",
    "master_feat = master_feat[master_feat['cat_unified'].isin(valid_cats)]\n",
    "\n",
    "# Label Encoding Kategori (Urut 0, 1, 2, 3...)\n",
    "le_cat = LabelEncoder()\n",
    "master_feat['cat_enc'] = le_cat.fit_transform(master_feat['cat_unified'])\n",
    "\n",
    "# ==========================================\n",
    "# 4. PELATIHAN MODEL\n",
    "# ==========================================\n",
    "lag_cols = [f'{c}_lag{i}' for c in pollutant_cols for i in [1, 2, 3]]\n",
    "roll_cols = [f'{c}_roll{w}' for c in pollutant_cols for w in [3, 7]]\n",
    "features_final = ['st_enc', 'month', 'day_of_week', 'is_weekend'] + lag_cols + roll_cols\n",
    "\n",
    "X = master_feat[features_final]\n",
    "y = master_feat['cat_enc']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"3/5 Melatih model HistGradientBoosting...\")\n",
    "clf = HistGradientBoostingClassifier(random_state=42, class_weight='balanced', max_iter=300)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluasi\n",
    "y_pred = clf.predict(X_val)\n",
    "print(f\"\\nMacro F1-Score: {f1_score(y_val, y_pred, average='macro'):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred, labels=np.arange(len(le_cat.classes_)), target_names=le_cat.classes_))\n",
    "\n",
    "# ==========================================\n",
    "# 5. GENERATE SUBMISSION (SINKRONISASI FITUR)\n",
    "# ==========================================\n",
    "print(\"\\n4/5 Sinkronisasi fitur untuk submission...\")\n",
    "sub = pd.read_csv(\"sample_submission.csv\")\n",
    "sub['dt'] = pd.to_datetime(sub['id'].str.split('_').str[0])\n",
    "sub['st_unified'] = sub['id'].str.split('_').str[1].apply(clean_stasiun)\n",
    "\n",
    "# Fitur Kalender & Encoding Stasiun\n",
    "sub['month'] = sub['dt'].dt.month\n",
    "sub['day_of_week'] = sub['dt'].dt.dayofweek\n",
    "sub['is_weekend'] = sub['day_of_week'].isin([5, 6]).astype(int)\n",
    "sub['st_enc'] = le_st.transform(sub['st_unified'])\n",
    "\n",
    "# Loop context window untuk mengambil lag dari data master\n",
    "sub_list = []\n",
    "for _, row in sub.iterrows():\n",
    "    # Ambil 7 hari terakhir dari master SEBELUM tanggal target\n",
    "    ctx = master[(master['st_unified'] == row['st_unified']) & (master['dt_final'] < row['dt'])].tail(7)\n",
    "    f_dict = {'id': row['id']}\n",
    "    for col in pollutant_cols:\n",
    "        f_dict[f'{col}_lag1'] = ctx[col].iloc[-1] if len(ctx) >= 1 else np.nan\n",
    "        f_dict[f'{col}_lag2'] = ctx[col].iloc[-2] if len(ctx) >= 2 else np.nan\n",
    "        f_dict[f'{col}_lag3'] = ctx[col].iloc[-3] if len(ctx) >= 3 else np.nan\n",
    "        f_dict[f'{col}_roll3'] = ctx[col].tail(3).mean() if len(ctx) >= 3 else np.nan\n",
    "        f_dict[f'{col}_roll7'] = ctx[col].tail(7).mean() if len(ctx) >= 7 else np.nan\n",
    "    sub_list.append(f_dict)\n",
    "\n",
    "sub_final_feat = sub.merge(pd.DataFrame(sub_list), on='id')\n",
    "sub['category'] = le_cat.inverse_transform(clf.predict(sub_final_feat[features_final]))\n",
    "\n",
    "# Save\n",
    "sub[['id', 'category']].to_csv(\"submission_xgb_final3.csv\", index=False)\n",
    "print(\"5/5 Selesai! File 'submission_advanced_vfinal.csv' telah siap.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aba988c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memuat data historis sesuai struktur jurnal...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_28544\\1391541890.py:48: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  return pd.to_datetime(val)\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_28544\\1391541890.py:48: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  return pd.to_datetime(val)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Membuat fitur Lag polutan (H-1)...\n",
      "Melatih model (XGBoost logic)...\n",
      "\n",
      "Macro F1-Score: 0.5865\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "              BAIK       0.44      0.55      0.49       122\n",
      "SANGAT TIDAK SEHAT       0.00      0.00      0.00         0\n",
      "            SEDANG       0.86      0.78      0.82       978\n",
      "       TIDAK SEHAT       0.39      0.54      0.45       159\n",
      "\n",
      "          accuracy                           0.73      1259\n",
      "         macro avg       0.42      0.47      0.44      1259\n",
      "      weighted avg       0.76      0.73      0.74      1259\n",
      "\n",
      "\n",
      "Membuat file submission...\n",
      "Selesai! File 'submission_journal_v1.csv' berhasil dibuat.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Andri\\Arkavidia10_Datavidia_GeprekMantan\\myvenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "d:\\Andri\\Arkavidia10_Datavidia_GeprekMantan\\myvenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "d:\\Andri\\Arkavidia10_Datavidia_GeprekMantan\\myvenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "d:\\Andri\\Arkavidia10_Datavidia_GeprekMantan\\myvenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "d:\\Andri\\Arkavidia10_Datavidia_GeprekMantan\\myvenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "d:\\Andri\\Arkavidia10_Datavidia_GeprekMantan\\myvenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "d:\\Andri\\Arkavidia10_Datavidia_GeprekMantan\\myvenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "d:\\Andri\\Arkavidia10_Datavidia_GeprekMantan\\myvenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "d:\\Andri\\Arkavidia10_Datavidia_GeprekMantan\\myvenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import os\n",
    "\n",
    "# ==========================================\n",
    "# 1. SETUP & DATA LOADING (Re-build Master)\n",
    "# ==========================================\n",
    "files_dict = {\n",
    "    2010: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2010-komponen-data.csv\",\n",
    "    2011: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2011-komponen-data.csv\",\n",
    "    2012: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2012-komponen-data.csv\",\n",
    "    2013: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2013-komponen-data.csv\",\n",
    "    2014: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2014-komponen-data.csv\",\n",
    "    2015: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2015-komponen-data.csv\",\n",
    "    2016: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2016-komponen-data.csv\",\n",
    "    2017: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2017-komponen-data.csv\",\n",
    "    2018: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2018-komponen-data.csv\",\n",
    "    2019: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2019-komponen-data.csv\",\n",
    "    2020: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2020-komponen-data.csv\",\n",
    "    2021: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2021-komponen-data.csv\",\n",
    "    2022: \"ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2022-komponen-data.csv\",\n",
    "    2023: \"ISPU/data-indeks-standar-pencemar-udara-(ispu)-di-provinsi-dki-jakarta-2023-komponen-data.csv\",\n",
    "    2024: \"ISPU/data-indeks-standar-pencemar-udara-(ispu)-di-provinsi-dki-jakarta-komponen-data-2024.csv\",\n",
    "    2025: \"data_ispu_2025.csv\"\n",
    "}\n",
    "\n",
    "pollutant_cols = ['pm_sepuluh', 'pm_duakomalima', 'sulfur_dioksida', 'karbon_monoksida', 'ozon', 'nitrogen_dioksida']\n",
    "rename_map = {'pm10': 'pm_sepuluh', 'pm_10': 'pm_sepuluh', 'pm25': 'pm_duakomalima', 'pm_25': 'pm_duakomalima',\n",
    "              'so2': 'sulfur_dioksida', 'co': 'karbon_monoksida', 'o3': 'ozon', 'no2': 'nitrogen_dioksida', 'lokasi_spku': 'stasiun'}\n",
    "\n",
    "def clean_stasiun(s):\n",
    "    s = str(s).upper()\n",
    "    for code in ['DKI1', 'DKI2', 'DKI3', 'DKI4', 'DKI5']:\n",
    "        if code in s: return code\n",
    "    return 'UNKNOWN'\n",
    "\n",
    "def parse_date(row, year):\n",
    "    try:\n",
    "        if year >= 2024:\n",
    "            return pd.to_datetime(str(int(row['periode_data'])) + str(int(row['tanggal'])).zfill(2), format='%Y%m%d')\n",
    "        val = row['tanggal']\n",
    "        if isinstance(val, (int, float)) or (isinstance(val, str) and str(val).replace('.','').isdigit()):\n",
    "            return pd.to_datetime('1899-12-30') + pd.to_timedelta(float(val), unit='D')\n",
    "        return pd.to_datetime(val)\n",
    "    except: return pd.NaT\n",
    "\n",
    "all_data = []\n",
    "print(\"Memuat data historis sesuai struktur jurnal...\")\n",
    "for year, filename in files_dict.items():\n",
    "    if not os.path.exists(filename): continue\n",
    "    df = pd.read_csv(filename).rename(columns=str.lower).rename(columns=rename_map)\n",
    "    df['dt_final'] = df.apply(lambda r: parse_date(r, year), axis=1)\n",
    "    df['st_unified'] = df['stasiun'].apply(clean_stasiun)\n",
    "    cat_col = next((c for c in ['kategori', 'categori'] if c in df.columns), None)\n",
    "    df['cat_unified'] = df[cat_col].astype(str).str.upper().str.strip() if cat_col else 'UNKNOWN'\n",
    "    \n",
    "    for c in pollutant_cols:\n",
    "        if c not in df.columns: df[c] = np.nan\n",
    "        else: df[c] = pd.to_numeric(df[c].astype(str).str.replace(',', '.').replace(['---', '-', '', 'N/A', 'NAN'], np.nan), errors='coerce')\n",
    "    all_data.append(df[['dt_final', 'st_unified', 'cat_unified'] + pollutant_cols])\n",
    "\n",
    "master = pd.concat(all_data, ignore_index=True).dropna(subset=['dt_final'])\n",
    "master = master[~master['cat_unified'].isin(['TIDAK ADA DATA', 'NAN', '', 'NONE', 'UNKNOWN'])]\n",
    "master = master[master['st_unified'] != 'UNKNOWN']\n",
    "\n",
    "# ==========================================\n",
    "# 2. FEATURE ENGINEERING (LAGGED POLLUTANTS)\n",
    "# ==========================================\n",
    "print(\"Membuat fitur Lag polutan (H-1)...\")\n",
    "master = master.sort_values(by=['st_unified', 'dt_final'])\n",
    "for col in pollutant_cols:\n",
    "    master[f'{col}_lag1'] = master.groupby('st_unified')[col].shift(1)\n",
    "\n",
    "master_lag = master.dropna(subset=[f'{c}_lag1' for c in pollutant_cols]).copy()\n",
    "\n",
    "# Filter kelas minimal 2 sampel (untuk stratify)\n",
    "cat_counts = master_lag['cat_unified'].value_counts()\n",
    "valid_cats = cat_counts[cat_counts >= 2].index\n",
    "master_lag = master_lag[master_lag['cat_unified'].isin(valid_cats)]\n",
    "\n",
    "# Label Encoding\n",
    "le_st = LabelEncoder()\n",
    "master_lag['st_enc'] = le_st.fit_transform(master_lag['st_unified'])\n",
    "le_cat = LabelEncoder()\n",
    "master_lag['cat_enc'] = le_cat.fit_transform(master_lag['cat_unified'])\n",
    "\n",
    "# ==========================================\n",
    "# 3. TRAINING (HIST-GRADIENT BOOSTING)\n",
    "# ==========================================\n",
    "# Fitur: Stasiun + 6 Polutan Utama (Sesuai Jurnal)\n",
    "features = ['st_enc'] + [f'{c}_lag1' for c in pollutant_cols]\n",
    "X = master_lag[features]\n",
    "y = master_lag['cat_enc']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Melatih model (XGBoost logic)...\")\n",
    "clf = HistGradientBoostingClassifier(\n",
    "    random_state=42, \n",
    "    class_weight='balanced', \n",
    "    max_iter=300,\n",
    "    learning_rate=0.05\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluasi\n",
    "y_pred = clf.predict(X_val)\n",
    "print(f\"\\nMacro F1-Score: {f1_score(y_val, y_pred, average='macro'):.4f}\")\n",
    "print(classification_report(y_val, y_pred, labels=np.arange(len(le_cat.classes_)), target_names=le_cat.classes_))\n",
    "\n",
    "# ==========================================\n",
    "# 4. SUBMISSION GENERATION\n",
    "# ==========================================\n",
    "print(\"\\nMembuat file submission...\")\n",
    "sub = pd.read_csv(\"sample_submission.csv\")\n",
    "sub['dt'] = pd.to_datetime(sub['id'].str.split('_').str[0])\n",
    "sub['st_unified'] = sub['id'].str.split('_').str[1].apply(clean_stasiun)\n",
    "sub['dt_yesterday'] = sub['dt'] - pd.Timedelta(days=1)\n",
    "\n",
    "# Lookup data polutan H-1 dari master data\n",
    "sub_final = sub.merge(master[['dt_final', 'st_unified'] + pollutant_cols], \n",
    "                      left_on=['dt_yesterday', 'st_unified'], \n",
    "                      right_on=['dt_final', 'st_unified'], how='left')\n",
    "\n",
    "# Mapping ke nama kolom fitur\n",
    "for c in pollutant_cols:\n",
    "    sub_final[f'{c}_lag1'] = sub_final[c]\n",
    "\n",
    "sub_final['st_enc'] = le_st.transform(sub_final['st_unified'])\n",
    "sub['category'] = le_cat.inverse_transform(clf.predict(sub_final[features]))\n",
    "\n",
    "sub[['id', 'category']].to_csv(\"submission_xgb_final4.csv\", index=False)\n",
    "print(\"Selesai! File 'submission_journal_v1.csv' berhasil dibuat.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c84d7af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv (3.11.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
